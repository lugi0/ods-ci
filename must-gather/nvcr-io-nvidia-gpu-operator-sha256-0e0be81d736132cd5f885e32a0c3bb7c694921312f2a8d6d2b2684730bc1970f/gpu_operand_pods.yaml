apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.34"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.34"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:26Z"
    generateName: 2f22aa8739212b008c9ce511a0427ff28188a7d69a3250b81b9294029907306-
    labels:
      controller-uid: 34cea9f3-5262-43b1-a14b-7501b4632c8c
      job-name: 2f22aa8739212b008c9ce511a0427ff28188a7d69a3250b81b9294029907306
    name: 2f22aa8739212b008c9ce511a0427ff28188a7d69a3250b81b929402994j65r
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: 2f22aa8739212b008c9ce511a0427ff28188a7d69a3250b81b9294029907306
      uid: 34cea9f3-5262-43b1-a14b-7501b4632c8c
    resourceVersion: "76498"
    uid: 67a5a6d5-e846-4b9d-a45d-82083cd22678
  spec:
    containers:
    - command:
      - opm
      - alpha
      - bundle
      - extract
      - -m
      - /bundle/
      - -n
      - redhat-nvidia-gpu-addon
      - -c
      - 2f22aa8739212b008c9ce511a0427ff28188a7d69a3250b81b9294029907306
      - -z
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/osd-addons/nvidia-gpu-addon-bundle@sha256:4e505e0472bccbe233a98f7f90faa68379088e21d2875dcaf10913d71f19268d
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imagePullPolicy: IfNotPresent
      name: extract
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ds5s4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    initContainers:
    - command:
      - /bin/cp
      - -Rv
      - /bin/cpb
      - /util/cpb
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imagePullPolicy: IfNotPresent
      name: util
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ds5s4
        readOnly: true
    - command:
      - /util/cpb
      - /bundle
      image: quay.io/osd-addons/nvidia-gpu-addon-bundle@sha256:4e505e0472bccbe233a98f7f90faa68379088e21d2875dcaf10913d71f19268d
      imagePullPolicy: Always
      name: pull
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ds5s4
        readOnly: true
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - emptyDir: {}
      name: bundle
    - emptyDir: {}
      name: util
    - name: kube-api-access-ds5s4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:32Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://7d9e64617122258d5571522c92b30ba450207fa0061141f24d7ad56caba79352
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      lastState: {}
      name: extract
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://7d9e64617122258d5571522c92b30ba450207fa0061141f24d7ad56caba79352
          exitCode: 0
          finishedAt: "2023-03-01T12:37:32Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:32Z"
    hostIP: 10.0.197.168
    initContainerStatuses:
    - containerID: cri-o://455942e274ba8c7e7ff1d27a8b5a990a1558b18067d5ab23ea0096c2a0071ea8
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      lastState: {}
      name: util
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://455942e274ba8c7e7ff1d27a8b5a990a1558b18067d5ab23ea0096c2a0071ea8
          exitCode: 0
          finishedAt: "2023-03-01T12:37:28Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:28Z"
    - containerID: cri-o://a635bc9ac00edf7b6799568f2b84fb91454064dea545a02b7602690083f83284
      image: quay.io/osd-addons/nvidia-gpu-addon-bundle@sha256:4e505e0472bccbe233a98f7f90faa68379088e21d2875dcaf10913d71f19268d
      imageID: quay.io/osd-addons/nvidia-gpu-addon-bundle@sha256:4e505e0472bccbe233a98f7f90faa68379088e21d2875dcaf10913d71f19268d
      lastState: {}
      name: pull
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://a635bc9ac00edf7b6799568f2b84fb91454064dea545a02b7602690083f83284
          exitCode: 0
          finishedAt: "2023-03-01T12:37:31Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:31Z"
    phase: Succeeded
    podIP: 10.128.2.34
    podIPs:
    - ip: 10.128.2.34
    qosClass: Burstable
    startTime: "2023-03-01T12:37:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.35"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.35"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:26Z"
    generateName: 5f5a51d644d1e13b2c35cb24e878d2a680f2003d88d73eb24c15ae6524a282a-
    labels:
      controller-uid: 331b4934-6dd5-4d08-af86-25278bf528c8
      job-name: 5f5a51d644d1e13b2c35cb24e878d2a680f2003d88d73eb24c15ae6524a282a
    name: 5f5a51d644d1e13b2c35cb24e878d2a680f2003d88d73eb24c15ae65248d7hr
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: 5f5a51d644d1e13b2c35cb24e878d2a680f2003d88d73eb24c15ae6524a282a
      uid: 331b4934-6dd5-4d08-af86-25278bf528c8
    resourceVersion: "76502"
    uid: dc23d84a-d44c-4d76-8cc3-519aa8bd3ccb
  spec:
    containers:
    - command:
      - opm
      - alpha
      - bundle
      - extract
      - -m
      - /bundle/
      - -n
      - redhat-nvidia-gpu-addon
      - -c
      - 5f5a51d644d1e13b2c35cb24e878d2a680f2003d88d73eb24c15ae6524a282a
      - -z
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/osd-addons/nvidia-gpu-addon-prometheus-bundle@sha256:577e7c4c2d8f78fdb8214ec10cf6b116feb6a902724001eeeffa5c98364874e0
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imagePullPolicy: IfNotPresent
      name: extract
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-phfmq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    initContainers:
    - command:
      - /bin/cp
      - -Rv
      - /bin/cpb
      - /util/cpb
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imagePullPolicy: IfNotPresent
      name: util
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-phfmq
        readOnly: true
    - command:
      - /util/cpb
      - /bundle
      image: quay.io/osd-addons/nvidia-gpu-addon-prometheus-bundle@sha256:577e7c4c2d8f78fdb8214ec10cf6b116feb6a902724001eeeffa5c98364874e0
      imagePullPolicy: Always
      name: pull
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-phfmq
        readOnly: true
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - emptyDir: {}
      name: bundle
    - emptyDir: {}
      name: util
    - name: kube-api-access-phfmq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:32Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://2b751441b1f0ba2c03773dde2091a818d5d80e5693141beed4c83a8e653e1097
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      lastState: {}
      name: extract
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://2b751441b1f0ba2c03773dde2091a818d5d80e5693141beed4c83a8e653e1097
          exitCode: 0
          finishedAt: "2023-03-01T12:37:32Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:32Z"
    hostIP: 10.0.197.168
    initContainerStatuses:
    - containerID: cri-o://fe582f5cfbd3409ea1b0b1bb31db567805095cbf41dbe6ef77cfa0f1104c6e04
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      lastState: {}
      name: util
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://fe582f5cfbd3409ea1b0b1bb31db567805095cbf41dbe6ef77cfa0f1104c6e04
          exitCode: 0
          finishedAt: "2023-03-01T12:37:29Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:28Z"
    - containerID: cri-o://564f00a6f3d9004784b79a9501d5ef242003431bc7dca71562853097c11d83aa
      image: quay.io/osd-addons/nvidia-gpu-addon-prometheus-bundle@sha256:577e7c4c2d8f78fdb8214ec10cf6b116feb6a902724001eeeffa5c98364874e0
      imageID: quay.io/osd-addons/nvidia-gpu-addon-prometheus-bundle@sha256:577e7c4c2d8f78fdb8214ec10cf6b116feb6a902724001eeeffa5c98364874e0
      lastState: {}
      name: pull
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://564f00a6f3d9004784b79a9501d5ef242003431bc7dca71562853097c11d83aa
          exitCode: 0
          finishedAt: "2023-03-01T12:37:31Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:31Z"
    phase: Succeeded
    podIP: 10.128.2.35
    podIPs:
    - ip: 10.128.2.35
    qosClass: Burstable
    startTime: "2023-03-01T12:37:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.29"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.29"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: anyuid
    creationTimestamp: "2023-03-01T12:37:02Z"
    generateName: addon-nvidia-gpu-addon-catalog-
    labels:
      olm.catalogSource: addon-nvidia-gpu-addon-catalog
      olm.pod-spec-hash: 77f4bd6cdd
    name: addon-nvidia-gpu-addon-catalog-9dqnx
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: operators.coreos.com/v1alpha1
      blockOwnerDeletion: false
      controller: false
      kind: CatalogSource
      name: addon-nvidia-gpu-addon-catalog
      uid: eac86912-9f3b-41b1-a105-26ebf0f16aef
    resourceVersion: "968284"
    uid: b0f5f9df-9aa0-4977-9bef-ec6b22ae217b
  spec:
    containers:
    - image: quay.io/osd-addons/nvidia-gpu-addon-index@sha256:709b65b1bd60b2a2896054384a5a02c99a75581cb2ec22d96f0e779f2db6d332
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - grpc_health_probe
          - -addr=:50051
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: registry-server
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - grpc_health_probe
          - -addr=:50051
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - MKNOD
        readOnlyRootFilesystem: false
      startupProbe:
        exec:
          command:
          - grpc_health_probe
          - -addr=:50051
        failureThreshold: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sqs4v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: addon-nvidia-gpu-addon-catalog-dockercfg-ccfdc
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: addon-nvidia-gpu-addon-catalog
    serviceAccountName: addon-nvidia-gpu-addon-catalog
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: kube-api-access-sqs4v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://842c8c699e6d287de5f240d2eb194c2256b982bc716da5bcdf2733e7faf1a3c7
      image: quay.io/osd-addons/nvidia-gpu-addon-index@sha256:709b65b1bd60b2a2896054384a5a02c99a75581cb2ec22d96f0e779f2db6d332
      imageID: quay.io/osd-addons/nvidia-gpu-addon-index@sha256:709b65b1bd60b2a2896054384a5a02c99a75581cb2ec22d96f0e779f2db6d332
      lastState: {}
      name: registry-server
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:08Z"
    hostIP: 10.0.197.168
    phase: Running
    podIP: 10.128.2.29
    podIPs:
    - ip: 10.128.2.29
    qosClass: Burstable
    startTime: "2023-03-01T12:37:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.5"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.5"
            ],
            "default": true,
            "dns": {}
        }]
      kubectl.kubernetes.io/default-container: alertmanager
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:59Z"
    generateName: alertmanager-gpuaddon-alertmanager-
    labels:
      alertmanager: gpuaddon-alertmanager
      app.kubernetes.io/instance: gpuaddon-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.24.0
      controller-revision-hash: alertmanager-gpuaddon-alertmanager-65cd6dcf77
      statefulset.kubernetes.io/pod-name: alertmanager-gpuaddon-alertmanager-0
    name: alertmanager-gpuaddon-alertmanager-0
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-gpuaddon-alertmanager
      uid: dcc92a68-4314-4720-bb10-d8d3f012f86b
    resourceVersion: "967016"
    uid: d74ebdbf-628b-48d4-8d74-f211c74a05c7
  spec:
    containers:
    - args:
      - --config.file=/etc/alertmanager/config/alertmanager.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=[$(POD_IP)]:9094
      - --web.listen-address=:9093
      - --web.route-prefix=/
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-0.alertmanager-operated:9094
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-1.alertmanager-operated:9094
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-2.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-gpuaddon-alertmanager-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wwvzr
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://localhost:9093/-/reload
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wwvzr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-gpuaddon-alertmanager-0
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app: alertmanager
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-gpuaddon-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-gpuaddon-alertmanager-tls-assets-0
    - emptyDir: {}
      name: alertmanager-gpuaddon-alertmanager-db
    - name: kube-api-access-wwvzr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://201059b78fc0756e129321db3c8da9422443bfd99267c9d79b73eb046cf07fb9
      image: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      imageID: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:89e69acc561cc7bea9a48b799ca9b08912c919f098393510f9a4599db8b21b5c
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:04Z"
    - containerID: cri-o://9cc75884d2abec6276918775fa81e3e0ff1f022e46450b622c9eae5d0a09e060
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imageID: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:05Z"
    hostIP: 10.0.197.168
    phase: Running
    podIP: 10.128.2.5
    podIPs:
    - ip: 10.128.2.5
    qosClass: Guaranteed
    startTime: "2023-03-01T12:37:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.9"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.9"
            ],
            "default": true,
            "dns": {}
        }]
      kubectl.kubernetes.io/default-container: alertmanager
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:59Z"
    generateName: alertmanager-gpuaddon-alertmanager-
    labels:
      alertmanager: gpuaddon-alertmanager
      app.kubernetes.io/instance: gpuaddon-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.24.0
      controller-revision-hash: alertmanager-gpuaddon-alertmanager-65cd6dcf77
      statefulset.kubernetes.io/pod-name: alertmanager-gpuaddon-alertmanager-1
    name: alertmanager-gpuaddon-alertmanager-1
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-gpuaddon-alertmanager
      uid: dcc92a68-4314-4720-bb10-d8d3f012f86b
    resourceVersion: "967078"
    uid: 0857e2e3-afc5-48b0-8f85-7a14e8e54094
  spec:
    containers:
    - args:
      - --config.file=/etc/alertmanager/config/alertmanager.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=[$(POD_IP)]:9094
      - --web.listen-address=:9093
      - --web.route-prefix=/
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-0.alertmanager-operated:9094
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-1.alertmanager-operated:9094
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-2.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-gpuaddon-alertmanager-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mfjsp
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://localhost:9093/-/reload
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mfjsp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-gpuaddon-alertmanager-1
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app: alertmanager
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-gpuaddon-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-gpuaddon-alertmanager-tls-assets-0
    - emptyDir: {}
      name: alertmanager-gpuaddon-alertmanager-db
    - name: kube-api-access-mfjsp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://d32ecc105a293641721b7d3e4561b6d7387bdbcb836fccf9cdc7eae05b1e39f9
      image: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      imageID: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:89e69acc561cc7bea9a48b799ca9b08912c919f098393510f9a4599db8b21b5c
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:04Z"
    - containerID: cri-o://e409d17e3ac70347a4244b25d4854e3bb29d6ce1ad61d0e0ba97db349ace9d8a
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imageID: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:05Z"
    hostIP: 10.0.197.168
    phase: Running
    podIP: 10.128.2.9
    podIPs:
    - ip: 10.128.2.9
    qosClass: Guaranteed
    startTime: "2023-03-01T12:37:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.6"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.6"
            ],
            "default": true,
            "dns": {}
        }]
      kubectl.kubernetes.io/default-container: alertmanager
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:59Z"
    generateName: alertmanager-gpuaddon-alertmanager-
    labels:
      alertmanager: gpuaddon-alertmanager
      app.kubernetes.io/instance: gpuaddon-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.24.0
      controller-revision-hash: alertmanager-gpuaddon-alertmanager-65cd6dcf77
      statefulset.kubernetes.io/pod-name: alertmanager-gpuaddon-alertmanager-2
    name: alertmanager-gpuaddon-alertmanager-2
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-gpuaddon-alertmanager
      uid: dcc92a68-4314-4720-bb10-d8d3f012f86b
    resourceVersion: "966271"
    uid: 9aa2609e-b862-4de5-83fa-b564d55f3578
  spec:
    containers:
    - args:
      - --config.file=/etc/alertmanager/config/alertmanager.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=[$(POD_IP)]:9094
      - --web.listen-address=:9093
      - --web.route-prefix=/
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-0.alertmanager-operated:9094
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-1.alertmanager-operated:9094
      - --cluster.peer=alertmanager-gpuaddon-alertmanager-2.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 100m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-gpuaddon-alertmanager-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wtp5p
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://localhost:9093/-/reload
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wtp5p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-gpuaddon-alertmanager-2
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    nodeName: ip-10-0-229-111.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app: alertmanager
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-gpuaddon-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-gpuaddon-alertmanager-tls-assets-0
    - emptyDir: {}
      name: alertmanager-gpuaddon-alertmanager-db
    - name: kube-api-access-wtp5p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://e0e81c11cc9dfd4f3cf4a3126cc003a905bab36d5a2ad7c2b035abf6c6fbaa4a
      image: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      imageID: registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:89e69acc561cc7bea9a48b799ca9b08912c919f098393510f9a4599db8b21b5c
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:42Z"
    - containerID: cri-o://d39349cce8635bb740be3f37db73f739f5b950c910d49d07e2cb9ac32cd71cd3
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imageID: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:43Z"
    hostIP: 10.0.229.111
    phase: Running
    podIP: 10.131.0.6
    podIPs:
    - ip: 10.131.0.6
    qosClass: Guaranteed
    startTime: "2023-03-01T12:37:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.43"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.43"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:38:38Z"
    generateName: console-plugin-nvidia-gpu-65c9679ffb-
    labels:
      app: console-plugin-nvidia-gpu
      pod-template-hash: 65c9679ffb
    name: console-plugin-nvidia-gpu-65c9679ffb-xhlrj
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: console-plugin-nvidia-gpu-65c9679ffb
      uid: 8f3df1be-c29c-42a5-908d-f682c8d4b9a6
    resourceVersion: "966001"
    uid: 98e51768-2d8f-469e-9a29-fc8738afe1a1
  spec:
    containers:
    - image: quay.io/edge-infrastructure/console-plugin-nvidia-gpu@sha256:3aee994ea1bb111b75bd40c5ce5ee9109591e9f14f09ead07ecb507b3f07e93e
      imagePullPolicy: Always
      name: console-plugin-nvidia-gpu
      ports:
      - containerPort: 9443
        protocol: TCP
      resources:
        limits:
          cpu: 20m
          memory: 200Mi
        requests:
          cpu: 10m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsNonRoot: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/serving-cert
        name: plugin-serving-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vhn4k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    nodeName: ip-10-0-229-111.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: plugin-serving-cert
      secret:
        defaultMode: 420
        secretName: plugin-serving-cert
    - configMap:
        defaultMode: 420
        name: nginx-conf
      name: nginx-conf
    - name: kube-api-access-vhn4k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://caec8602ed23e4a33fa9f8f5dba42e77e8d3d1d200edaaff9dcfd800c70c5dcf
      image: quay.io/edge-infrastructure/console-plugin-nvidia-gpu@sha256:3aee994ea1bb111b75bd40c5ce5ee9109591e9f14f09ead07ecb507b3f07e93e
      imageID: quay.io/edge-infrastructure/console-plugin-nvidia-gpu@sha256:3aee994ea1bb111b75bd40c5ce5ee9109591e9f14f09ead07ecb507b3f07e93e
      lastState: {}
      name: console-plugin-nvidia-gpu
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:58Z"
    hostIP: 10.0.229.111
    phase: Running
    podIP: 10.131.0.43
    podIPs:
    - ip: 10.131.0.43
    qosClass: Burstable
    startTime: "2023-03-01T12:38:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "nvidia.addons.rh-ecosystem-edge.io/v1alpha1",
            "kind": "GPUAddon",
            "metadata": {
              "name": "gpuaddon-config-sample",
              "namespace": "redhat-nvidia-gpu-addon"
            },
            "spec": {
              "console_plugin_enabled": true
            }
          }
        ]
      capabilities: Basic Install
      certified: "false"
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.11"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.11"
            ],
            "default": true,
            "dns": {}
        }]
      kubectl.kubernetes.io/default-container: manager
      olm.operatorGroup: redhat-layered-product-og
      olm.operatorNamespace: redhat-nvidia-gpu-addon
      olm.skipRange: '>=0.0.1 <1.2.2'
      olm.targetNamespaces: redhat-nvidia-gpu-addon
      openshift.io/scc: restricted
      operatorframework.io/properties: '{"properties":[{"type":"olm.package","value":{"packageName":"nvidia-gpu-addon-operator","version":"1.2.2"}},{"type":"olm.gvk","value":{"group":"nvidia.addons.rh-ecosystem-edge.io","kind":"GPUAddon","version":"v1alpha1"}},{"type":"olm.gvk","value":{"group":"nvidia.addons.rh-ecosystem-edge.io","kind":"Monitoring","version":"v1alpha1"}},{"type":"olm.package.required","value":{"packageName":"ose-nfd","versionRange":"4.10.0"}},{"type":"olm.package.required","value":{"packageName":"ose-prometheus-operator","versionRange":"4.10.0"}}]}'
      operatorframework.io/suggested-namespace: redhat-nvidia-gpu-addon
      operators.operatorframework.io/builder: operator-sdk-v1.22.2
      operators.operatorframework.io/project_layout: go.kubebuilder.io/v3
      repository: https://github.com/rh-ecosystem-edge/nvidia-gpu-addon-operator
      support: Red Hat
    creationTimestamp: "2023-03-01T12:37:43Z"
    generateName: controller-manager-6c88c48689-
    labels:
      control-plane: controller-manager
      pod-template-hash: 6c88c48689
    name: controller-manager-6c88c48689-tdzm8
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: controller-manager-6c88c48689
      uid: f3d34e61-e4bb-48d5-86af-dd5f6d721c98
    resourceVersion: "968944"
    uid: 35e9b897-5f16-4ab3-b9ed-20b94cb67204
  spec:
    containers:
    - args:
      - --secure-listen-address=0.0.0.0:8443
      - --upstream=http://127.0.0.1:8080/
      - --logtostderr=true
      - --v=0
      env:
      - name: OPERATOR_CONDITION_NAME
        value: nvidia-gpu-addon-operator.1.2.2
      image: registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:01c7cb194032e5488b4d20e29a2b474a1bfa24984893e625e2544432b950f461
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        tcpSocket:
          port: 8443
        timeoutSeconds: 1
      name: kube-rbac-proxy
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 8443
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 5m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4hpqh
        readOnly: true
    - args:
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
      env:
      - name: RELATED_IMAGE_CONSOLE_PLUGIN
        value: quay.io/edge-infrastructure/console-plugin-nvidia-gpu@sha256:3aee994ea1bb111b75bd40c5ce5ee9109591e9f14f09ead07ecb507b3f07e93e
      - name: RELATED_IMAGE_MUST_GATHER
        value: quay.io/edge-infrastructure/nvidia-gpu-addon-operator-must-gather:1.2
      - name: OPERATOR_CONDITION_NAME
        value: nvidia-gpu-addon-operator.1.2.2
      image: quay.io/edge-infrastructure/nvidia-gpu-addon-operator:1.2
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4hpqh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: controller-manager-dockercfg-2sk5v
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      runAsNonRoot: true
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: controller-manager
    serviceAccountName: controller-manager
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: kube-api-access-4hpqh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://f065df1ba9fc4fd19e0c3f50fa5bf4f471fdc718b13849188f1ce954c2d2676d
      image: registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:01c7cb194032e5488b4d20e29a2b474a1bfa24984893e625e2544432b950f461
      imageID: registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:01c7cb194032e5488b4d20e29a2b474a1bfa24984893e625e2544432b950f461
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:04Z"
    - containerID: cri-o://7bbcf72a20fdd9fdbfab7c0bc2b46c5cd727a8f8615cd953c6522ddebffdd826
      image: quay.io/edge-infrastructure/nvidia-gpu-addon-operator:1.2
      imageID: quay.io/edge-infrastructure/nvidia-gpu-addon-operator@sha256:d9adda45921f1d4514e38588d1564fb2faf54696257f707230efd33258c4d573
      lastState: {}
      name: manager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:05Z"
    hostIP: 10.0.197.168
    phase: Running
    podIP: 10.128.2.11
    podIPs:
    - ip: 10.128.2.11
    qosClass: Burstable
    startTime: "2023-03-01T12:37:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.65"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.65"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:38:00Z"
    generateName: d4c3b85e112da75dcb84fe52e18e7caceab09876e985e083c8905e46da8909d-
    labels:
      controller-uid: 773e9a39-e787-4d31-865f-7cc2b7cd25d9
      job-name: d4c3b85e112da75dcb84fe52e18e7caceab09876e985e083c8905e46da8909d
    name: d4c3b85e112da75dcb84fe52e18e7caceab09876e985e083c8905e46dawsfwk
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: d4c3b85e112da75dcb84fe52e18e7caceab09876e985e083c8905e46da8909d
      uid: 773e9a39-e787-4d31-865f-7cc2b7cd25d9
    resourceVersion: "78184"
    uid: 69f7075c-84ea-434d-b058-3c07f2b50c05
  spec:
    containers:
    - command:
      - opm
      - alpha
      - bundle
      - extract
      - -m
      - /bundle/
      - -n
      - redhat-nvidia-gpu-addon
      - -c
      - d4c3b85e112da75dcb84fe52e18e7caceab09876e985e083c8905e46da8909d
      - -z
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/osd-addons/nvidia-gpu-addon-gpu-operator-certified-bundle@sha256:741dbd7e8355eec70c440e6cb68a99487639da653c31f3b85b038f29601670a1
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imagePullPolicy: IfNotPresent
      name: extract
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5txj5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    initContainers:
    - command:
      - /bin/cp
      - -Rv
      - /bin/cpb
      - /util/cpb
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imagePullPolicy: IfNotPresent
      name: util
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5txj5
        readOnly: true
    - command:
      - /util/cpb
      - /bundle
      image: quay.io/osd-addons/nvidia-gpu-addon-gpu-operator-certified-bundle@sha256:741dbd7e8355eec70c440e6cb68a99487639da653c31f3b85b038f29601670a1
      imagePullPolicy: Always
      name: pull
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5txj5
        readOnly: true
    nodeName: ip-10-0-229-111.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - emptyDir: {}
      name: bundle
    - emptyDir: {}
      name: util
    - name: kube-api-access-5txj5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:10Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:00Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:00Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://347b14923d4722df0772405af3bfa1eebb2082deade43afdce8153b19020514e
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      lastState: {}
      name: extract
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://347b14923d4722df0772405af3bfa1eebb2082deade43afdce8153b19020514e
          exitCode: 0
          finishedAt: "2023-03-01T12:38:15Z"
          reason: Completed
          startedAt: "2023-03-01T12:38:15Z"
    hostIP: 10.0.229.111
    initContainerStatuses:
    - containerID: cri-o://01524afddc8234ca203cea85f269f564157c93216efe07207e49f0362988ed0c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      lastState: {}
      name: util
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://01524afddc8234ca203cea85f269f564157c93216efe07207e49f0362988ed0c
          exitCode: 0
          finishedAt: "2023-03-01T12:38:08Z"
          reason: Completed
          startedAt: "2023-03-01T12:38:08Z"
    - containerID: cri-o://02d7f1cde83a494c45abf03653d998338d14149fce6447adf111ea4851e4f57a
      image: quay.io/osd-addons/nvidia-gpu-addon-gpu-operator-certified-bundle@sha256:741dbd7e8355eec70c440e6cb68a99487639da653c31f3b85b038f29601670a1
      imageID: quay.io/osd-addons/nvidia-gpu-addon-gpu-operator-certified-bundle@sha256:741dbd7e8355eec70c440e6cb68a99487639da653c31f3b85b038f29601670a1
      lastState: {}
      name: pull
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://02d7f1cde83a494c45abf03653d998338d14149fce6447adf111ea4851e4f57a
          exitCode: 0
          finishedAt: "2023-03-01T12:38:10Z"
          reason: Completed
          startedAt: "2023-03-01T12:38:10Z"
    phase: Succeeded
    podIP: 10.131.0.65
    podIPs:
    - ip: 10.131.0.65
    qosClass: Burstable
    startTime: "2023-03-01T12:38:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.36"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.36"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:26Z"
    generateName: df7f4da7d2f12435b906c21f643c01e07d357cf810f29a05483546554f23f34-
    labels:
      controller-uid: c41e2131-af96-47a4-9013-8a1b663cebc8
      job-name: df7f4da7d2f12435b906c21f643c01e07d357cf810f29a05483546554f23f34
    name: df7f4da7d2f12435b906c21f643c01e07d357cf810f29a05483546554f69b4f
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: df7f4da7d2f12435b906c21f643c01e07d357cf810f29a05483546554f23f34
      uid: c41e2131-af96-47a4-9013-8a1b663cebc8
    resourceVersion: "76505"
    uid: 00329652-a3cb-4980-828b-e35187ea26b6
  spec:
    containers:
    - command:
      - opm
      - alpha
      - bundle
      - extract
      - -m
      - /bundle/
      - -n
      - redhat-nvidia-gpu-addon
      - -c
      - df7f4da7d2f12435b906c21f643c01e07d357cf810f29a05483546554f23f34
      - -z
      env:
      - name: CONTAINER_IMAGE
        value: quay.io/osd-addons/nvidia-gpu-addon-nfd-bundle@sha256:d66cdf10c63439c778031606d3b1c63c92d4ebfd051580a66429151c36e64c1a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imagePullPolicy: IfNotPresent
      name: extract
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x55v6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: default-dockercfg-2c6bf
    initContainers:
    - command:
      - /bin/cp
      - -Rv
      - /bin/cpb
      - /util/cpb
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imagePullPolicy: IfNotPresent
      name: util
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x55v6
        readOnly: true
    - command:
      - /util/cpb
      - /bundle
      image: quay.io/osd-addons/nvidia-gpu-addon-nfd-bundle@sha256:d66cdf10c63439c778031606d3b1c63c92d4ebfd051580a66429151c36e64c1a
      imagePullPolicy: Always
      name: pull
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bundle
        name: bundle
      - mountPath: /util
        name: util
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x55v6
        readOnly: true
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - emptyDir: {}
      name: bundle
    - emptyDir: {}
      name: util
    - name: kube-api-access-x55v6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:32Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://6cbcf963486daacf63a576f79dbbe512f539e549718cc837c9cd6b60b07d874a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a85fbe0aecf2203c347074c95cae8ef611897c5c614e2ea8ad3a612b32ab355b
      lastState: {}
      name: extract
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://6cbcf963486daacf63a576f79dbbe512f539e549718cc837c9cd6b60b07d874a
          exitCode: 0
          finishedAt: "2023-03-01T12:37:32Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:32Z"
    hostIP: 10.0.197.168
    initContainerStatuses:
    - containerID: cri-o://de27aab05d517fb84bcde3bae8897bc9b0a55c0e68b674e087419456d12a1d1c
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:58d171c685b8bba98ed0a0497b23ddc9703fe975ce93e04182a283ac17f31410
      lastState: {}
      name: util
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://de27aab05d517fb84bcde3bae8897bc9b0a55c0e68b674e087419456d12a1d1c
          exitCode: 0
          finishedAt: "2023-03-01T12:37:28Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:28Z"
    - containerID: cri-o://19a72ea15c3ea27b7dbab09160d47331d28bd49e0d01841a950ebaab89c0a0ef
      image: quay.io/osd-addons/nvidia-gpu-addon-nfd-bundle@sha256:d66cdf10c63439c778031606d3b1c63c92d4ebfd051580a66429151c36e64c1a
      imageID: quay.io/osd-addons/nvidia-gpu-addon-nfd-bundle@sha256:d66cdf10c63439c778031606d3b1c63c92d4ebfd051580a66429151c36e64c1a
      lastState: {}
      name: pull
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://19a72ea15c3ea27b7dbab09160d47331d28bd49e0d01841a950ebaab89c0a0ef
          exitCode: 0
          finishedAt: "2023-03-01T12:37:31Z"
          reason: Completed
          startedAt: "2023-03-01T12:37:31Z"
    phase: Succeeded
    podIP: 10.128.2.36
    podIPs:
    - ip: 10.128.2.36
    qosClass: Burstable
    startTime: "2023-03-01T12:37:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.16"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.16"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-gpu-feature-discovery
    creationTimestamp: "2023-03-02T16:14:22Z"
    generateName: gpu-feature-discovery-
    labels:
      app: gpu-feature-discovery
      app.kubernetes.io/part-of: nvidia-gpu
      controller-revision-hash: 89cf89dbd
      pod-template-generation: "1"
    name: gpu-feature-discovery-pb6lc
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: gpu-feature-discovery
      uid: f7f7b3cd-78ff-45d8-958f-51e02023c39e
    resourceVersion: "972638"
    uid: c9ce8d92-c2fd-4590-b0e2-5a1faa2ac67e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - env:
      - name: GFD_SLEEP_INTERVAL
        value: 60s
      - name: GFD_FAIL_ON_INIT_ERROR
        value: "true"
      - name: MIG_STRATEGY
        value: single
      - name: NVIDIA_MIG_MONITOR_DEVICES
        value: all
      image: nvcr.io/nvidia/gpu-feature-discovery@sha256:bec9f026d9b3d9404c78d6091817a359015c6a7aa411735b34138c1518853b5d
      imagePullPolicy: IfNotPresent
      name: gpu-feature-discovery
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: output-dir
      - mountPath: /sys/class/dmi/id/product_name
        name: dmi-product-name
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d4ks4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nvidia-gpu-feature-discovery-dockercfg-qff5m
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d4ks4
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.gpu-feature-discovery: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-gpu-feature-discovery
    serviceAccountName: nvidia-gpu-feature-discovery
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: output-dir
    - hostPath:
        path: /sys/class/dmi/id/product_name
        type: ""
      name: dmi-product-name
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - name: kube-api-access-d4ks4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://462bd83e2786525cedf324a2f00dfaf4b08a97d4e033af279f806df617952fce
      image: nvcr.io/nvidia/gpu-feature-discovery@sha256:bec9f026d9b3d9404c78d6091817a359015c6a7aa411735b34138c1518853b5d
      imageID: nvcr.io/nvidia/gpu-feature-discovery@sha256:408a0886e19ecc872d56c070e08d6dfc67c7e32fd2e63f2e4776ec2f3b9700e4
      lastState: {}
      name: gpu-feature-discovery
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:15:42Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://70e6d186aed8b4ad23ee3bf21b1e66fb43e7639708b55ff7a89637e05c5e4583
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: toolkit-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://70e6d186aed8b4ad23ee3bf21b1e66fb43e7639708b55ff7a89637e05c5e4583
          exitCode: 0
          finishedAt: "2023-03-02T16:15:42Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:26Z"
    phase: Running
    podIP: 10.131.2.16
    podIPs:
    - ip: 10.131.2.16
    qosClass: BestEffort
    startTime: "2023-03-02T16:14:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "nvidia.com/v1",
            "kind": "ClusterPolicy",
            "metadata": {
              "name": "gpu-cluster-policy"
            },
            "spec": {
              "operator": {
                "defaultRuntime": "crio",
                "use_ocp_driver_toolkit": true,
                "initContainer": {
                }
              },
              "sandboxWorkloads": {
                "enabled": false,
                "defaultWorkload": "container"
              },
              "driver": {
                "enabled": true,
                "upgradePolicy": {
                  "autoUpgrade": true,
                  "drain": {
                    "deleteEmptyDir": false,
                    "enable": false,
                    "force": false,
                    "timeoutSeconds": 300
                  },
                  "maxParallelUpgrades": 1,
                  "podDeletion": {
                    "deleteEmptyDir": false,
                    "force": false,
                    "timeoutSeconds": 300
                  },
                  "waitForCompletion": {
                    "timeoutSeconds": 0
                  }
                },
                "repoConfig": {
                  "configMapName": ""
                },
                "certConfig": {
                  "name": ""
                },
                "licensingConfig": {
                  "nlsEnabled": false,
                  "configMapName": ""
                },
                "virtualTopology": {
                  "config": ""
                },
                "kernelModuleConfig": {
                  "name": ""
                }
              },
              "dcgmExporter": {
                "enabled": true,
                "config": {
                  "name": ""
                },
                "serviceMonitor": {
                  "enabled": true
                }
              },
              "dcgm": {
                "enabled": true
              },
              "daemonsets": {
                "updateStrategy": "RollingUpdate",
                "rollingUpdate": {
                  "maxUnavailable": "1"
                }
              },
              "devicePlugin": {
                "enabled": true,
                "config": {
                  "name": "",
                  "default": ""
                }
              },
              "gfd": {
                "enabled": true
              },
              "migManager": {
                "enabled": true
              },
              "nodeStatusExporter": {
                "enabled": true
              },
              "mig": {
                "strategy": "single"
              },
              "toolkit": {
                "enabled": true
              },
              "validator": {
                "plugin": {
                  "env": [
                    {
                      "name": "WITH_WORKLOAD",
                      "value": "true"
                    }
                  ]
                }
              },
              "vgpuManager": {
                "enabled": false
              },
              "vgpuDeviceManager": {
                "enabled": true
              },
              "sandboxDevicePlugin": {
                "enabled": true
              },
              "vfioManager": {
                "enabled": true
              }
            }
          }
        ]
      capabilities: Basic Install
      categories: AI/Machine Learning, OpenShift Optional
      certified: "true"
      containerImage: nvcr.io/nvidia/gpu-operator@sha256:54d5846c0291db621931a8544c9cb6437512470a93278ca9130ba73f2e2f0055
      createdAt: Mon Jan 30 15:43:24 PST 2023
      description: Automate the management and monitoring of NVIDIA GPUs.
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.46"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.46"
            ],
            "default": true,
            "dns": {}
        }]
      olm.operatorGroup: redhat-layered-product-og
      olm.operatorNamespace: redhat-nvidia-gpu-addon
      olm.skipRange: '>=1.9.0 <22.9.2'
      olm.targetNamespaces: redhat-nvidia-gpu-addon
      openshift.io/scc: hostmount-anyuid
      operatorframework.io/properties: '{"properties":[{"type":"olm.package","value":{"packageName":"gpu-operator-certified","version":"22.9.2"}},{"type":"olm.gvk","value":{"group":"nvidia.com","kind":"ClusterPolicy","version":"v1"}}]}'
      operatorframework.io/suggested-namespace: nvidia-gpu-operator
      operators.openshift.io/infrastructure-features: '["Disconnected"]'
      operators.operatorframework.io/builder: operator-sdk-v1.4.0
      operators.operatorframework.io/project_layout: go.kubebuilder.io/v3
      provider: NVIDIA
      repository: http://github.com/NVIDIA/gpu-operator
      support: NVIDIA
    creationTimestamp: "2023-03-01T12:38:34Z"
    generateName: gpu-operator-b4f74c8cf-
    labels:
      app: gpu-operator
      app.kubernetes.io/component: gpu-operator
      nvidia.com/gpu-driver-upgrade-drain.skip: "true"
      pod-template-hash: b4f74c8cf
    name: gpu-operator-b4f74c8cf-vz6lr
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gpu-operator-b4f74c8cf
      uid: 47740b51-dbdf-46a5-9c6c-cc90d5c52ec3
    resourceVersion: "968598"
    uid: 51f7aaf4-9125-4d2f-8493-f12a2e47cd64
  spec:
    containers:
    - args:
      - --leader-elect
      - --leader-lease-renew-deadline
      - 60s
      command:
      - gpu-operator
      env:
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VALIDATOR_IMAGE
        value: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      - name: GFD_IMAGE
        value: nvcr.io/nvidia/gpu-feature-discovery@sha256:bec9f026d9b3d9404c78d6091817a359015c6a7aa411735b34138c1518853b5d
      - name: CONTAINER_TOOLKIT_IMAGE
        value: nvcr.io/nvidia/k8s/container-toolkit@sha256:efb88937f73434994d1bbadc87b492a1df047aa9f8d6e9f5ec3b09536e6e7691
      - name: DCGM_IMAGE
        value: nvcr.io/nvidia/cloud-native/dcgm@sha256:c3cf59dd5d6160eba5d816ade2e81b35ebb10f4884df67971f6ace36f8e6efc1
      - name: DCGM_EXPORTER_IMAGE
        value: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:9a00cdfdddb73327ef8e8e0fa60e50926a388d380c551bba6a3d3012be40401d
      - name: DEVICE_PLUGIN_IMAGE
        value: nvcr.io/nvidia/k8s-device-plugin@sha256:9c17d3a907eb77eb8f7b4f3faf52d8352e4252af92003f828083f80d629bd2c3
      - name: DRIVER_IMAGE
        value: nvcr.io/nvidia/driver@sha256:268c26781c46b36fe691638e807eb5c3a2cd077029ddb095ac75264adac04925
      - name: DRIVER_IMAGE-515
        value: nvcr.io/nvidia/driver@sha256:f250d1e4989fd4821e81bad9227eeeaa531ca8810f3a40e996f1ca2af7945f96
      - name: DRIVER_IMAGE-510
        value: nvcr.io/nvidia/driver@sha256:f2ca4612f3418e0e3963be8c3771ffe463cdc1fc1b810437a3aad9d76bb6499d
      - name: DRIVER_IMAGE-470
        value: nvcr.io/nvidia/driver@sha256:c5a62bc2a6cce339798e4b7cf9cec47e289c2d30b40f451ba8c9e958a68e3e55
      - name: DRIVER_IMAGE-450
        value: nvcr.io/nvidia/driver@sha256:b860ef04cc6cd8c481810fa19bef7221071f5a3862640b2ad6c2704728dc77c8
      - name: DRIVER_MANAGER_IMAGE
        value: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:9177a0ae30798b42d0387f6a20cd3ce3cd1799a91b7866bf812368764b05b1af
      - name: MIG_MANAGER_IMAGE
        value: nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:50c2a2b998e467c82716efa79fe136aa6f7ea95fd23576cf384d251bb9628640
      - name: CUDA_BASE_IMAGE
        value: nvcr.io/nvidia/cuda@sha256:5f2a2d8977f2c59abe88394f05cc3c044194554f90994d8554b0e1117ab5900d
      - name: VFIO_MANAGER_IMAGE
        value: nvcr.io/nvidia/cuda@sha256:5f2a2d8977f2c59abe88394f05cc3c044194554f90994d8554b0e1117ab5900d
      - name: SANDBOX_DEVICE_PLUGIN_IMAGE
        value: nvcr.io/nvidia/kubevirt-gpu-device-plugin@sha256:0d47dad29d2ef445b301c5c64717758eed43a606345b79f97bce2e64b40a91a8
      - name: VGPU_DEVICE_MANAGER_IMAGE
        value: nvcr.io/nvidia/cloud-native/vgpu-device-manager@sha256:64d757b4c80b910e64647a84a1d592fab2ea3313ff6dce30c25c3a08e180bd74
      - name: OPERATOR_CONDITION_NAME
        value: gpu-operator-certified.v22.9.2
      image: nvcr.io/nvidia/gpu-operator@sha256:54d5846c0291db621931a8544c9cb6437512470a93278ca9130ba73f2e2f0055
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: gpu-operator
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - MKNOD
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dc5wm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: gpu-operator-dockercfg-glqjq
    nodeName: ip-10-0-229-111.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: gpu-operator
    serviceAccountName: gpu-operator
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - name: kube-api-access-dc5wm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://8b951f6fe991c89689f379c7579176cbdec854e456c5b6433e1015548b74d824
      image: nvcr.io/nvidia/gpu-operator@sha256:54d5846c0291db621931a8544c9cb6437512470a93278ca9130ba73f2e2f0055
      imageID: nvcr.io/nvidia/gpu-operator@sha256:0e0be81d736132cd5f885e32a0c3bb7c694921312f2a8d6d2b2684730bc1970f
      lastState: {}
      name: gpu-operator
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:59Z"
    hostIP: 10.0.229.111
    phase: Running
    podIP: 10.131.0.46
    podIPs:
    - ip: 10.131.0.46
    qosClass: Burstable
    startTime: "2023-03-01T12:38:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "nfd.openshift.io/v1",
            "kind": "NodeFeatureDiscovery",
            "metadata": {
              "name": "nfd-instance",
              "namespace": "openshift-nfd"
            },
            "spec": {
              "customConfig": {
                "configData": "#    - name: \"more.kernel.features\"\n#      matchOn:\n#      - loadedKMod: [\"example_kmod3\"]\n#    - name: \"more.features.by.nodename\"\n#      value: customValue\n#      matchOn:\n#      - nodename: [\"special-.*-node-.*\"]\n"
              },
              "operand": {
                "image": "registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b",
                "servicePort": 12000
              },
              "workerConfig": {
                "configData": "core:\n#  labelWhiteList:\n#  noPublish: false\n  sleepInterval: 60s\n#  sources: [all]\n#  klog:\n#    addDirHeader: false\n#    alsologtostderr: false\n#    logBacktraceAt:\n#    logtostderr: true\n#    skipHeaders: false\n#    stderrthreshold: 2\n#    v: 0\n#    vmodule:\n##   NOTE: the following options are not dynamically run-time \n##          configurable and require a nfd-worker restart to take effect\n##          after being changed\n#    logDir:\n#    logFile:\n#    logFileMaxSize: 1800\n#    skipLogHeaders: false\nsources:\n#  cpu:\n#    cpuid:\n##     NOTE: whitelist has priority over blacklist\n#      attributeBlacklist:\n#        - \"BMI1\"\n#        - \"BMI2\"\n#        - \"CLMUL\"\n#        - \"CMOV\"\n#        - \"CX16\"\n#        - \"ERMS\"\n#        - \"F16C\"\n#        - \"HTT\"\n#        - \"LZCNT\"\n#        - \"MMX\"\n#        - \"MMXEXT\"\n#        - \"NX\"\n#        - \"POPCNT\"\n#        - \"RDRAND\"\n#        - \"RDSEED\"\n#        - \"RDTSCP\"\n#        - \"SGX\"\n#        - \"SSE\"\n#        - \"SSE2\"\n#        - \"SSE3\"\n#        - \"SSE4.1\"\n#        - \"SSE4.2\"\n#        - \"SSSE3\"\n#      attributeWhitelist:\n#  kernel:\n#    kconfigFile: \"/path/to/kconfig\"\n#    configOpts:\n#      - \"NO_HZ\"\n#      - \"X86\"\n#      - \"DMI\"\n  pci:\n    deviceClassWhitelist:\n      - \"0200\"\n      - \"03\"\n      - \"12\"\n    deviceLabelFields:\n#      - \"class\"\n      - \"vendor\"\n#      - \"device\"\n#      - \"subsystem_vendor\"\n#      - \"subsystem_device\"\n#  usb:\n#    deviceClassWhitelist:\n#      - \"0e\"\n#      - \"ef\"\n#      - \"fe\"\n#      - \"ff\"\n#    deviceLabelFields:\n#      - \"class\"\n#      - \"vendor\"\n#      - \"device\"\n#  custom:\n#    - name: \"my.kernel.feature\"\n#      matchOn:\n#        - loadedKMod: [\"example_kmod1\", \"example_kmod2\"]\n#    - name: \"my.pci.feature\"\n#      matchOn:\n#        - pciId:\n#            class: [\"0200\"]\n#            vendor: [\"15b3\"]\n#            device: [\"1014\", \"1017\"]\n#        - pciId :\n#            vendor: [\"8086\"]\n#            device: [\"1000\", \"1100\"]\n#    - name: \"my.usb.feature\"\n#      matchOn:\n#        - usbId:\n#          class: [\"ff\"]\n#          vendor: [\"03e7\"]\n#          device: [\"2485\"]\n#        - usbId:\n#          class: [\"fe\"]\n#          vendor: [\"1a6e\"]\n#          device: [\"089a\"]\n#    - name: \"my.combined.feature\"\n#      matchOn:\n#        - pciId:\n#            vendor: [\"15b3\"]\n#            device: [\"1014\", \"1017\"]\n#          loadedKMod : [\"vendor_kmod1\", \"vendor_kmod2\"]\n"
              }
            }
          },
          {
            "apiVersion": "nfd.openshift.io/v1alpha1",
            "kind": "NodeFeatureRule",
            "metadata": {
              "name": "my-sample-rule-object"
            },
            "spec": {
              "rules": [
                {
                  "labels": {
                    "my-sample-feature": "true"
                  },
                  "matchFeatures": [
                    {
                      "feature": "kernel.loadedmodule",
                      "matchExpressions": {
                        "dummy": {
                          "op": "Exists"
                        }
                      }
                    },
                    {
                      "feature": "kernel.config",
                      "matchExpressions": {
                        "X86": {
                          "op": "In",
                          "value": [
                            "y"
                          ]
                        }
                      }
                    }
                  ],
                  "name": "my sample rule"
                }
              ]
            }
          }
        ]
      capabilities: Deep Insights
      categories: Integration & Delivery,OpenShift Optional
      containerImage: registry.redhat.io/openshift4/ose-cluster-nfd-operator@sha256:0dcf04b3bdaeaf63d5e5a510ba0dd828d332da4d7d3ddcd75a94a0f3c7723c89
      description: |-
        The Node Feature Discovery Operator manages the detection of hardware features and configuration in a Kubernetes cluster by labeling the nodes with hardware-specific information. The Node Feature Discovery (NFD) will label the host with node-specific attributes, like PCI cards, kernel, or OS version, and many more.

        NFD consists  of the following software components:

        The NFD Operator is based on the Operator Framework an open source toolkit to manage Kubernetes native applications, called Operators, in an effective, automated, and scalable way.

        ##NFD-Master
        NFD-Master is the daemon responsible for communication towards the Kubernetes API. That is, it receives labeling requests from the worker and modifies node objects accordingly.

        ##NFD-Worker
        NFD-Worker is a daemon responsible for feature detection. It then communicates the information to nfd-master which does the actual node labeling. One instance of nfd-worker is supposed to be running on each node of the cluster.

        ##NFD-Topology-Updater
        NFD-Topology-Updater is a daemon responsible for examining allocated resources on a worker node to account for resources available to be allocated to new pod on a per-zone basis (where a zone can be a NUMA node). It then communicates the information to nfd-master which does the NodeResourceTopology CR creation corresponding to all the nodes in the cluster. One instance of nfd-topology-updater is supposed to be running on each node of the cluster.
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.34"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.0.34"
            ],
            "default": true,
            "dns": {}
        }]
      olm.operatorGroup: redhat-layered-product-og
      olm.operatorNamespace: redhat-nvidia-gpu-addon
      olm.skipRange: '>=4.6.0 <4.10.0'
      olm.targetNamespaces: redhat-nvidia-gpu-addon
      openshift.io/scc: anyuid
      operatorframework.io/cluster-monitoring: "true"
      operatorframework.io/properties: '{"properties":[{"type":"olm.package","value":{"packageName":"ose-nfd","version":"4.10.0"}},{"type":"olm.gvk","value":{"group":"nfd.openshift.io","kind":"NodeFeatureDiscovery","version":"v1"}},{"type":"olm.gvk","value":{"group":"nfd.openshift.io","kind":"NodeFeatureRule","version":"v1alpha1"}},{"type":"olm.gvk","value":{"group":"topology.node.k8s.io","kind":"NodeResourceTopology","version":"v1alpha1"}}]}'
      operatorframework.io/suggested-namespace: openshift-nfd
      operators.openshift.io/infrastructure-features: '["disconnected"]'
      operators.operatorframework.io/builder: operator-sdk-v1.16.0+git
      operators.operatorframework.io/internal-objects: '["noderesourcetopologies.topology.node.k8s.io"]'
      operators.operatorframework.io/project_layout: go.kubebuilder.io/v3
      provider: Red Hat
      repository: https://github.com/openshift/cluster-nfd-operator
      support: Red Hat
    creationTimestamp: "2023-03-01T12:37:49Z"
    generateName: nfd-controller-manager-59c56864f9-
    labels:
      control-plane: controller-manager
      pod-template-hash: 59c56864f9
    name: nfd-controller-manager-59c56864f9-fbhk8
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nfd-controller-manager-59c56864f9
      uid: b17a1458-6428-41d3-96cd-5f6b383d0ee1
    resourceVersion: "966725"
    uid: 9a2e5005-fbbf-490f-8cd6-62fdb2bad504
  spec:
    containers:
    - args:
      - --secure-listen-address=0.0.0.0:8443
      - --upstream=http://127.0.0.1:8080/
      - --logtostderr=true
      - --v=10
      - --tls-cert-file=/etc/secrets/tls.crt
      - --tls-private-key-file=/etc/secrets/tls.key
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
      env:
      - name: OPERATOR_CONDITION_NAME
        value: ose-nfd.4.10.0
      image: registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:01c7cb194032e5488b4d20e29a2b474a1bfa24984893e625e2544432b950f461
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 250m
          memory: 64Mi
      securityContext:
        capabilities:
          drop:
          - MKNOD
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/secrets
        name: node-feature-discovery-operator-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vbrpt
        readOnly: true
    - args:
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
      command:
      - /node-feature-discovery-operator
      env:
      - name: SSL_CERT_DIR
        value: /etc/pki/tls/certs
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OPERATOR_NAME
        value: cluster-nfd-operator
      - name: NODE_FEATURE_DISCOVERY_IMAGE
        value: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      - name: OPERATOR_CONDITION_NAME
        value: ose-nfd.4.10.0
      image: quay.io/edge-infrastructure/cluster-nfd-operator@sha256:795feb27604fd2a5909655ae814c6c521006afe64ee68a1967ac402436b5a480
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - MKNOD
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vbrpt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nfd-operator-dockercfg-4q6d7
    nodeName: ip-10-0-229-111.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: nfd-operator
    serviceAccountName: nfd-operator
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: node-feature-discovery-operator-tls
      secret:
        defaultMode: 420
        secretName: node-feature-discovery-operator-tls
    - name: kube-api-access-vbrpt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://5616d12c2dcf4eae954f2e4b2951f6ace4869864d73c3d2c1540ab2775715bde
      image: registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:01c7cb194032e5488b4d20e29a2b474a1bfa24984893e625e2544432b950f461
      imageID: registry.redhat.io/openshift4/ose-kube-rbac-proxy@sha256:01c7cb194032e5488b4d20e29a2b474a1bfa24984893e625e2544432b950f461
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:48Z"
    - containerID: cri-o://45b5d2361b195b28b955040fc6f677cc480c850774e378b6e921c7107522d4be
      image: quay.io/edge-infrastructure/cluster-nfd-operator@sha256:795feb27604fd2a5909655ae814c6c521006afe64ee68a1967ac402436b5a480
      imageID: quay.io/edge-infrastructure/cluster-nfd-operator@sha256:795feb27604fd2a5909655ae814c6c521006afe64ee68a1967ac402436b5a480
      lastState: {}
      name: manager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:52Z"
    hostIP: 10.0.229.111
    phase: Running
    podIP: 10.131.0.34
    podIPs:
    - ip: 10.131.0.34
    qosClass: Burstable
    startTime: "2023-03-01T12:37:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.0.15"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.129.0.15"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-master-
    labels:
      app: nfd-master
      controller-revision-hash: 5d474c54dd
      pod-template-generation: "1"
    name: nfd-master-lqnlt
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-master
      uid: 73050428-b1e9-41e4-85e6-dd3777c3fa8c
    resourceVersion: "965588"
    uid: c630f718-09f2-4a85-b0da-6bc0c1c574d1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-213-93.us-east-2.compute.internal
    containers:
    - args:
      - --port=12000
      command:
      - nfd-master
      - --extra-label-ns=nvidia.com
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-master
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fjw2h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nfd-master-dockercfg-2bwpt
    nodeName: ip-10-0-213-93.us-east-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/master: ""
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: nfd-master
    serviceAccountName: nfd-master
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Equal
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-fjw2h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://d69eb16d511beaf9d6107d146f33da59b69e718fc0455f10b513e88f28fb4a09
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-master
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:51Z"
    hostIP: 10.0.213.93
    phase: Running
    podIP: 10.129.0.15
    podIPs:
    - ip: 10.129.0.15
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.0.17"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.130.0.17"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-master-
    labels:
      app: nfd-master
      controller-revision-hash: 5d474c54dd
      pod-template-generation: "1"
    name: nfd-master-qhqxn
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-master
      uid: 73050428-b1e9-41e4-85e6-dd3777c3fa8c
    resourceVersion: "966116"
    uid: 7ca8eea2-4f54-43a1-9783-968fb8b6f065
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-90.us-east-2.compute.internal
    containers:
    - args:
      - --port=12000
      command:
      - nfd-master
      - --extra-label-ns=nvidia.com
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-master
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pmfml
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nfd-master-dockercfg-2bwpt
    nodeName: ip-10-0-193-90.us-east-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/master: ""
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: nfd-master
    serviceAccountName: nfd-master
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Equal
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-pmfml
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://cb380d2f9649d5b0b22c27f4976245d7fbc0e51ef54cb15549995618e508f010
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-master
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:58Z"
    hostIP: 10.0.193.90
    phase: Running
    podIP: 10.130.0.17
    podIPs:
    - ip: 10.130.0.17
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.0.5"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.0.5"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-master-
    labels:
      app: nfd-master
      controller-revision-hash: 5d474c54dd
      pod-template-generation: "1"
    name: nfd-master-z5tpb
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-master
      uid: 73050428-b1e9-41e4-85e6-dd3777c3fa8c
    resourceVersion: "967212"
    uid: 1a04c53a-acc7-4827-98e6-dc82649927ef
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-231-170.us-east-2.compute.internal
    containers:
    - args:
      - --port=12000
      command:
      - nfd-master
      - --extra-label-ns=nvidia.com
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-master
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zcvld
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nfd-master-dockercfg-2bwpt
    nodeName: ip-10-0-231-170.us-east-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/master: ""
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: nfd-master
    serviceAccountName: nfd-master
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Equal
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: kube-api-access-zcvld
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://215e1499e4366e56ee6c4bc139acf12cde1fbe2dd04d0fcad8b5b2ab5db5fdbc
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-master
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:09Z"
    hostIP: 10.0.231.170
    phase: Running
    podIP: 10.128.0.5
    podIPs:
    - ip: 10.128.0.5
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nfd-worker
    creationTimestamp: "2023-03-01T12:38:52Z"
    generateName: nfd-worker-
    labels:
      app: nfd-worker
      controller-revision-hash: 79c8d6f99f
      pod-template-generation: "1"
    name: nfd-worker-58jk2
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-worker
      uid: 2e97e3be-afd9-40c3-94d2-9698a22741a6
    resourceVersion: "963514"
    uid: 8251b906-f452-4b6d-baef-1ed8827bb7f3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - args:
      - --server=nfd-master:$(NFD_MASTER_SERVICE_PORT)
      command:
      - nfd-worker
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-worker
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-boot
        name: host-boot
        readOnly: true
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /host-sys
        name: host-sys
      - mountPath: /host-usr/lib
        name: host-usr-lib
        readOnly: true
      - mountPath: /host-usr/src
        name: host-usr-src
        readOnly: true
      - mountPath: /etc/kubernetes/node-feature-discovery
        name: nfd-worker-config
      - mountPath: /etc/kubernetes/node-feature-discovery/source.d
        name: nfd-hooks
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: nfd-features
      - mountPath: /etc/kubernetes/node-feature-discovery/custom.d/custom-rules
        name: custom-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l6flm
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: nfd-worker-dockercfg-8mjcp
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
      supplementalGroups:
      - 1001040000
    serviceAccount: nfd-worker
    serviceAccountName: nfd-worker
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot
        type: ""
      name: host-boot
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /usr/lib
        type: ""
      name: host-usr-lib
    - hostPath:
        path: /usr/src
        type: ""
      name: host-usr-src
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/source.d
        type: ""
      name: nfd-hooks
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: nfd-features
    - configMap:
        defaultMode: 420
        items:
        - key: nfd-worker-conf
          path: nfd-worker.conf
        name: nfd-worker
      name: nfd-worker-config
    - configMap:
        defaultMode: 420
        items:
        - key: custom-conf
          path: custom.conf
        name: nfd-worker
      name: custom-config
    - name: kube-api-access-l6flm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://99b3e100397058feda4e09f79a2b4379fe7ccc5f85e8d3423dac6c6acd844d99
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-worker
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:34Z"
    hostIP: 10.0.193.255
    phase: Running
    podIP: 10.0.193.255
    podIPs:
    - ip: 10.0.193.255
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nfd-worker
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-worker-
    labels:
      app: nfd-worker
      controller-revision-hash: 79c8d6f99f
      pod-template-generation: "1"
    name: nfd-worker-9nq2l
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-worker
      uid: 2e97e3be-afd9-40c3-94d2-9698a22741a6
    resourceVersion: "963113"
    uid: be46a7c0-4a38-49c2-9f47-3a7268e1b504
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-246-19.us-east-2.compute.internal
    containers:
    - args:
      - --server=nfd-master:$(NFD_MASTER_SERVICE_PORT)
      command:
      - nfd-worker
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-worker
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-boot
        name: host-boot
        readOnly: true
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /host-sys
        name: host-sys
      - mountPath: /host-usr/lib
        name: host-usr-lib
        readOnly: true
      - mountPath: /host-usr/src
        name: host-usr-src
        readOnly: true
      - mountPath: /etc/kubernetes/node-feature-discovery
        name: nfd-worker-config
      - mountPath: /etc/kubernetes/node-feature-discovery/source.d
        name: nfd-hooks
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: nfd-features
      - mountPath: /etc/kubernetes/node-feature-discovery/custom.d/custom-rules
        name: custom-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qbdbl
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ip-10-0-246-19.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
      supplementalGroups:
      - 1001040000
    serviceAccount: nfd-worker
    serviceAccountName: nfd-worker
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot
        type: ""
      name: host-boot
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /usr/lib
        type: ""
      name: host-usr-lib
    - hostPath:
        path: /usr/src
        type: ""
      name: host-usr-src
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/source.d
        type: ""
      name: nfd-hooks
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: nfd-features
    - configMap:
        defaultMode: 420
        items:
        - key: nfd-worker-conf
          path: nfd-worker.conf
        name: nfd-worker
      name: nfd-worker-config
    - configMap:
        defaultMode: 420
        items:
        - key: custom-conf
          path: custom.conf
        name: nfd-worker
      name: custom-config
    - name: kube-api-access-qbdbl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://1e7b6ace6080a15d72e92eb6052bc4660ee38cbb80d5906d59c0ea6642d58fab
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-worker
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:24Z"
    hostIP: 10.0.246.19
    phase: Running
    podIP: 10.0.246.19
    podIPs:
    - ip: 10.0.246.19
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nfd-worker
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-worker-
    labels:
      app: nfd-worker
      controller-revision-hash: 79c8d6f99f
      pod-template-generation: "1"
    name: nfd-worker-qp4lx
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-worker
      uid: 2e97e3be-afd9-40c3-94d2-9698a22741a6
    resourceVersion: "966152"
    uid: 37443e7a-f0c7-4452-bd36-33647c8ca53c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-197-168.us-east-2.compute.internal
    containers:
    - args:
      - --server=nfd-master:$(NFD_MASTER_SERVICE_PORT)
      command:
      - nfd-worker
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-worker
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-boot
        name: host-boot
        readOnly: true
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /host-sys
        name: host-sys
      - mountPath: /host-usr/lib
        name: host-usr-lib
        readOnly: true
      - mountPath: /host-usr/src
        name: host-usr-src
        readOnly: true
      - mountPath: /etc/kubernetes/node-feature-discovery
        name: nfd-worker-config
      - mountPath: /etc/kubernetes/node-feature-discovery/source.d
        name: nfd-hooks
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: nfd-features
      - mountPath: /etc/kubernetes/node-feature-discovery/custom.d/custom-rules
        name: custom-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-r6bmv
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
      supplementalGroups:
      - 1001040000
    serviceAccount: nfd-worker
    serviceAccountName: nfd-worker
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot
        type: ""
      name: host-boot
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /usr/lib
        type: ""
      name: host-usr-lib
    - hostPath:
        path: /usr/src
        type: ""
      name: host-usr-src
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/source.d
        type: ""
      name: nfd-hooks
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: nfd-features
    - configMap:
        defaultMode: 420
        items:
        - key: nfd-worker-conf
          path: nfd-worker.conf
        name: nfd-worker
      name: nfd-worker-config
    - configMap:
        defaultMode: 420
        items:
        - key: custom-conf
          path: custom.conf
        name: nfd-worker
      name: custom-config
    - name: kube-api-access-r6bmv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://6718c82b83e49c4b76491671cf69cc4b56024370aadf67d8c6b201f9d2b61acc
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-worker
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:59Z"
    hostIP: 10.0.197.168
    phase: Running
    podIP: 10.0.197.168
    podIPs:
    - ip: 10.0.197.168
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nfd-worker
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-worker-
    labels:
      app: nfd-worker
      controller-revision-hash: 79c8d6f99f
      pod-template-generation: "1"
    name: nfd-worker-sztb7
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-worker
      uid: 2e97e3be-afd9-40c3-94d2-9698a22741a6
    resourceVersion: "963938"
    uid: f69baf32-9fbb-4f20-bb52-720b3e3bb9fa
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-229-111.us-east-2.compute.internal
    containers:
    - args:
      - --server=nfd-master:$(NFD_MASTER_SERVICE_PORT)
      command:
      - nfd-worker
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-worker
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-boot
        name: host-boot
        readOnly: true
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /host-sys
        name: host-sys
      - mountPath: /host-usr/lib
        name: host-usr-lib
        readOnly: true
      - mountPath: /host-usr/src
        name: host-usr-src
        readOnly: true
      - mountPath: /etc/kubernetes/node-feature-discovery
        name: nfd-worker-config
      - mountPath: /etc/kubernetes/node-feature-discovery/source.d
        name: nfd-hooks
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: nfd-features
      - mountPath: /etc/kubernetes/node-feature-discovery/custom.d/custom-rules
        name: custom-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f59wj
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: nfd-worker-dockercfg-8mjcp
    nodeName: ip-10-0-229-111.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
      supplementalGroups:
      - 1001040000
    serviceAccount: nfd-worker
    serviceAccountName: nfd-worker
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot
        type: ""
      name: host-boot
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /usr/lib
        type: ""
      name: host-usr-lib
    - hostPath:
        path: /usr/src
        type: ""
      name: host-usr-src
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/source.d
        type: ""
      name: nfd-hooks
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: nfd-features
    - configMap:
        defaultMode: 420
        items:
        - key: nfd-worker-conf
          path: nfd-worker.conf
        name: nfd-worker
      name: nfd-worker-config
    - configMap:
        defaultMode: 420
        items:
        - key: custom-conf
          path: custom.conf
        name: nfd-worker
      name: custom-config
    - name: kube-api-access-f59wj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://0afe509f28327d65bd681d0bc307371452969f9f54fe078ab516c76242f1164d
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-worker
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:37Z"
    hostIP: 10.0.229.111
    phase: Running
    podIP: 10.0.229.111
    podIPs:
    - ip: 10.0.229.111
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nfd-worker
    creationTimestamp: "2023-03-01T12:38:06Z"
    generateName: nfd-worker-
    labels:
      app: nfd-worker
      controller-revision-hash: 79c8d6f99f
      pod-template-generation: "1"
    name: nfd-worker-vj44c
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nfd-worker
      uid: 2e97e3be-afd9-40c3-94d2-9698a22741a6
    resourceVersion: "963210"
    uid: 56cc9dcd-22ac-4f1a-830a-7d5a6d872f79
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-209-134.us-east-2.compute.internal
    containers:
    - args:
      - --server=nfd-master:$(NFD_MASTER_SERVICE_PORT)
      command:
      - nfd-worker
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imagePullPolicy: Always
      name: nfd-worker
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-boot
        name: host-boot
        readOnly: true
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /host-sys
        name: host-sys
      - mountPath: /host-usr/lib
        name: host-usr-lib
        readOnly: true
      - mountPath: /host-usr/src
        name: host-usr-src
        readOnly: true
      - mountPath: /etc/kubernetes/node-feature-discovery
        name: nfd-worker-config
      - mountPath: /etc/kubernetes/node-feature-discovery/source.d
        name: nfd-hooks
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: nfd-features
      - mountPath: /etc/kubernetes/node-feature-discovery/custom.d/custom-rules
        name: custom-config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bcrf6
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ip-10-0-209-134.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
      supplementalGroups:
      - 1001040000
    serviceAccount: nfd-worker
    serviceAccountName: nfd-worker
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot
        type: ""
      name: host-boot
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /usr/lib
        type: ""
      name: host-usr-lib
    - hostPath:
        path: /usr/src
        type: ""
      name: host-usr-src
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/source.d
        type: ""
      name: nfd-hooks
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: nfd-features
    - configMap:
        defaultMode: 420
        items:
        - key: nfd-worker-conf
          path: nfd-worker.conf
        name: nfd-worker
      name: nfd-worker-config
    - configMap:
        defaultMode: 420
        items:
        - key: custom-conf
          path: custom.conf
        name: nfd-worker
      name: custom-config
    - name: kube-api-access-bcrf6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://90d0c7a1a1e19e497d48ce98e38bf85faf3c3fd57f3f602a97a3189d6553686c
      image: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:edd2adfdf423d6a1eb7e8c1e388d9cf5fbc829e7e66c7bc955e9b2a6f50d1a47
      imageID: registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:e24f1f2f82c0286a305940011d117def21bd9bcd481b00c0ebc3143fe15ad864
      lastState: {}
      name: nfd-worker
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:30Z"
    hostIP: 10.0.209.134
    phase: Running
    podIP: 10.0.209.134
    podIPs:
    - ip: 10.0.209.134
    qosClass: BestEffort
    startTime: "2023-03-01T12:38:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.18"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.18"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: privileged
    creationTimestamp: "2023-03-02T16:14:21Z"
    generateName: nvidia-container-toolkit-daemonset-
    labels:
      app: nvidia-container-toolkit-daemonset
      controller-revision-hash: 5c6fccb484
      pod-template-generation: "1"
    name: nvidia-container-toolkit-daemonset-fxdlr
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-container-toolkit-daemonset
      uid: 92e221ed-af5e-498e-a632-a5a340311ac4
    resourceVersion: "971601"
    uid: 4ec948d9-3138-4120-b02f-9589eccdf58c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - args:
      - '[[ -f /run/nvidia/validations/host-driver-ready ]] && driver_root=/ || driver_root=/run/nvidia/driver;
        export NVIDIA_DRIVER_ROOT=$driver_root; sleep 5; exec nvidia-toolkit /usr/local/nvidia'
      command:
      - bash
      - -c
      env:
      - name: RUNTIME_ARGS
      - name: RUNTIME
        value: crio
      image: nvcr.io/nvidia/k8s/container-toolkit@sha256:efb88937f73434994d1bbadc87b492a1df047aa9f8d6e9f5ec3b09536e6e7691
      imagePullPolicy: IfNotPresent
      name: nvidia-container-toolkit-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: nvidia-run-path
      - mountPath: /usr/local/nvidia
        name: toolkit-install-dir
      - mountPath: /usr/share/containers/oci/hooks.d
        name: crio-hooks
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4t6h7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostPID: true
    imagePullSecrets:
    - name: nvidia-container-toolkit-dockercfg-sc77j
    initContainers:
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "true"
      - name: COMPONENT
        value: driver
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: driver-validation
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/driver
        mountPropagation: HostToContainer
        name: driver-install-path
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /host-dev-char
        name: host-dev-char
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4t6h7
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.container-toolkit: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-container-toolkit
    serviceAccountName: nvidia-container-toolkit
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: DirectoryOrCreate
      name: nvidia-run-path
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /run/nvidia/driver
        type: ""
      name: driver-install-path
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /usr/local/nvidia
        type: ""
      name: toolkit-install-dir
    - hostPath:
        path: /run/containers/oci/hooks.d
        type: ""
      name: crio-hooks
    - hostPath:
        path: /dev/char
        type: ""
      name: host-dev-char
    - name: kube-api-access-4t6h7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://0df24b7c611598ce27a0c6f021fc9bfa241567da2b744879a9c9e867e45c32f9
      image: nvcr.io/nvidia/k8s/container-toolkit@sha256:efb88937f73434994d1bbadc87b492a1df047aa9f8d6e9f5ec3b09536e6e7691
      imageID: nvcr.io/nvidia/k8s/container-toolkit@sha256:8d4d704ba750acc1011e00516df376b11a950360dba34d3a97a04c34086eefb0
      lastState: {}
      name: nvidia-container-toolkit-ctr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:15:13Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://bb00c9a876f6bb1b94cad55436b3c7d67ae42a3d6b40597214f2198df67e4143
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: driver-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://bb00c9a876f6bb1b94cad55436b3c7d67ae42a3d6b40597214f2198df67e4143
          exitCode: 0
          finishedAt: "2023-03-02T16:15:12Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:27Z"
    phase: Running
    podIP: 10.131.2.18
    podIPs:
    - ip: 10.131.2.18
    qosClass: BestEffort
    startTime: "2023-03-02T16:14:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.19"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.19"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-operator-validator
    creationTimestamp: "2023-03-02T16:15:37Z"
    generateName: nvidia-cuda-validator-
    labels:
      app: nvidia-cuda-validator
    name: nvidia-cuda-validator-hv9nq
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: nvidia.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ClusterPolicy
      name: ocp-gpu-addon
      uid: eaa7f72e-7226-4afc-8d7e-ca5787b7a154
    resourceVersion: "972664"
    uid: ac03ae91-5694-49d3-b9e2-7989bcc01f44
  spec:
    containers:
    - args:
      - echo cuda workload validation is successful
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: nvidia-cuda-validator
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jcrsc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nvidia-operator-validator-dockercfg-fd76m
    initContainers:
    - args:
      - vectorAdd
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: cuda-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jcrsc
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-operator-validator
    serviceAccountName: nvidia-operator-validator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-jcrsc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:41Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:37Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:37Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://05820f7a65636877032be1babf28b0a9b0408ebdea0a3173082d75d62c81c0b7
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: nvidia-cuda-validator
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://05820f7a65636877032be1babf28b0a9b0408ebdea0a3173082d75d62c81c0b7
          exitCode: 0
          finishedAt: "2023-03-02T16:15:41Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:41Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://9bd6729b6bff0e599b77f78f8fa9fa20084cc65ffe6eeeb6ed84a96e5b2c5835
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: cuda-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://9bd6729b6bff0e599b77f78f8fa9fa20084cc65ffe6eeeb6ed84a96e5b2c5835
          exitCode: 0
          finishedAt: "2023-03-02T16:15:40Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:40Z"
    phase: Succeeded
    podIP: 10.131.2.19
    podIPs:
    - ip: 10.131.2.19
    qosClass: BestEffort
    startTime: "2023-03-02T16:15:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nvidia-dcgm-exporter
    creationTimestamp: "2023-03-02T16:14:23Z"
    generateName: nvidia-dcgm-exporter-
    labels:
      app: nvidia-dcgm-exporter
      controller-revision-hash: b9ff6b96c
      pod-template-generation: "1"
    name: nvidia-dcgm-exporter-5mprj
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dcgm-exporter
      uid: cfdfaf85-e628-428f-868f-6a4301e257da
    resourceVersion: "972635"
    uid: a6d6629e-4590-443f-b890-cfc4db9f1e7e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - env:
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
      - name: DCGM_REMOTE_HOSTENGINE_INFO
        value: localhost:5555
      image: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:9a00cdfdddb73327ef8e8e0fa60e50926a388d380c551bba6a3d3012be40401d
      imagePullPolicy: IfNotPresent
      name: nvidia-dcgm-exporter
      ports:
      - containerPort: 9400
        hostPort: 9400
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kxjwj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: nvidia-dcgm-exporter-dockercfg-9d9kx
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kxjwj
        readOnly: true
    - command:
      - /bin/entrypoint.sh
      env:
      - name: NVIDIA_DISABLE_REQUIRE
        value: "true"
      image: nvcr.io/nvidia/cuda@sha256:5f2a2d8977f2c59abe88394f05cc3c044194554f90994d8554b0e1117ab5900d
      imagePullPolicy: IfNotPresent
      name: init-pod-nvidia-node-status-exporter
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
      - mountPath: /bin/entrypoint.sh
        name: init-config
        readOnly: true
        subPath: entrypoint.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kxjwj
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.dcgm-exporter: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-dcgm-exporter
    serviceAccountName: nvidia-dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - hostPath:
        path: /run/nvidia
        type: ""
      name: run-nvidia
    - configMap:
        defaultMode: 448
        name: nvidia-dcgm-exporter
      name: init-config
    - name: kube-api-access-kxjwj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://21465c381a767441b999d41a306243de68c6671326fcb32e7147cff97431d833
      image: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:9a00cdfdddb73327ef8e8e0fa60e50926a388d380c551bba6a3d3012be40401d
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:9a00cdfdddb73327ef8e8e0fa60e50926a388d380c551bba6a3d3012be40401d
      lastState: {}
      name: nvidia-dcgm-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:15:42Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://3391b79783d9b784b690af280e451459cb9feaf210f714ebe07beb1614069287
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: toolkit-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://3391b79783d9b784b690af280e451459cb9feaf210f714ebe07beb1614069287
          exitCode: 0
          finishedAt: "2023-03-02T16:15:40Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:25Z"
    - containerID: cri-o://5f3d17c96883c2f40e63a128c4f344f7eef0bde77f82580e19345c4af37c3fec
      image: nvcr.io/nvidia/cuda@sha256:5f2a2d8977f2c59abe88394f05cc3c044194554f90994d8554b0e1117ab5900d
      imageID: nvcr.io/nvidia/cuda@sha256:0582a03385184d484d2da34d24dee7ae62028cd33ea0bff589575a243266ae02
      lastState: {}
      name: init-pod-nvidia-node-status-exporter
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://5f3d17c96883c2f40e63a128c4f344f7eef0bde77f82580e19345c4af37c3fec
          exitCode: 0
          finishedAt: "2023-03-02T16:15:41Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:41Z"
    phase: Running
    podIP: 10.0.193.255
    podIPs:
    - ip: 10.0.193.255
    qosClass: BestEffort
    startTime: "2023-03-02T16:14:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      openshift.io/scc: nvidia-dcgm
    creationTimestamp: "2023-03-02T16:14:21Z"
    generateName: nvidia-dcgm-
    labels:
      app: nvidia-dcgm
      controller-revision-hash: f47d857cb
      pod-template-generation: "1"
    name: nvidia-dcgm-gn8cf
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dcgm
      uid: 7bfa336a-ea74-4de1-b2c1-28ea0b134de1
    resourceVersion: "972614"
    uid: 84fca8e5-0a87-4a6a-9798-7227cd4c79f4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - image: nvcr.io/nvidia/cloud-native/dcgm@sha256:c3cf59dd5d6160eba5d816ade2e81b35ebb10f4884df67971f6ace36f8e6efc1
      imagePullPolicy: IfNotPresent
      name: nvidia-dcgm-ctr
      ports:
      - containerPort: 5555
        hostPort: 5555
        name: dcgm
        protocol: TCP
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5x56b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    imagePullSecrets:
    - name: nvidia-dcgm-dockercfg-bj746
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5x56b
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.dcgm: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-dcgm
    serviceAccountName: nvidia-dcgm
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - name: kube-api-access-5x56b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://bada46c71273b1fee4a3247652979f23900b964aeb3ba883ab184041e650b8fc
      image: nvcr.io/nvidia/cloud-native/dcgm@sha256:c3cf59dd5d6160eba5d816ade2e81b35ebb10f4884df67971f6ace36f8e6efc1
      imageID: nvcr.io/nvidia/cloud-native/dcgm@sha256:32ed4b68b8b9ba431db81804e9872d2066dbe82276520afaeb11af2642b6f486
      lastState: {}
      name: nvidia-dcgm-ctr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:15:41Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://5853c9718cebf795cda875b1cf23aa4dd17e857344ec827514801e747d02828d
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: toolkit-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://5853c9718cebf795cda875b1cf23aa4dd17e857344ec827514801e747d02828d
          exitCode: 0
          finishedAt: "2023-03-02T16:15:40Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:25Z"
    phase: Running
    podIP: 10.0.193.255
    podIPs:
    - ip: 10.0.193.255
    qosClass: BestEffort
    startTime: "2023-03-02T16:14:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.15"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.15"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: privileged
    creationTimestamp: "2023-03-02T16:14:21Z"
    generateName: nvidia-device-plugin-daemonset-
    labels:
      app: nvidia-device-plugin-daemonset
      controller-revision-hash: 84b477d858
      pod-template-generation: "1"
    name: nvidia-device-plugin-daemonset-vcdf5
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-device-plugin-daemonset
      uid: 57f9ee0d-c8a1-45fa-bc90-d467c7f84f42
    resourceVersion: "972608"
    uid: 58cae03a-0c4b-4d71-800b-058911d95e02
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - args:
      - '[[ -f /run/nvidia/validations/host-driver-ready ]] && driver_root=/ || driver_root=/run/nvidia/driver;
        export NVIDIA_DRIVER_ROOT=$driver_root; exec nvidia-device-plugin;'
      command:
      - bash
      - -c
      env:
      - name: PASS_DEVICE_SPECS
        value: "true"
      - name: FAIL_ON_INIT_ERROR
        value: "true"
      - name: DEVICE_LIST_STRATEGY
        value: envvar
      - name: DEVICE_ID_STRATEGY
        value: uuid
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: all
      - name: MIG_STRATEGY
        value: single
      - name: NVIDIA_MIG_MONITOR_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s-device-plugin@sha256:9c17d3a907eb77eb8f7b4f3faf52d8352e4252af92003f828083f80d629bd2c3
      imagePullPolicy: IfNotPresent
      name: nvidia-device-plugin
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/device-plugins
        name: device-plugin
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qsrmp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nvidia-device-plugin-dockercfg-hh7zh
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qsrmp
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.device-plugin: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-device-plugin
    serviceAccountName: nvidia-device-plugin
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/device-plugins
        type: ""
      name: device-plugin
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - name: kube-api-access-qsrmp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://849a60d801014b54355baac5a650d8c06d6d69fffdec7efa65b12380f0bee83a
      image: nvcr.io/nvidia/k8s-device-plugin@sha256:9c17d3a907eb77eb8f7b4f3faf52d8352e4252af92003f828083f80d629bd2c3
      imageID: nvcr.io/nvidia/k8s-device-plugin@sha256:3eff6b814aa67f4cab2776e8333a8d94dbee8d55d715244cf444329848cdffcb
      lastState: {}
      name: nvidia-device-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:15:41Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://af08b676bd435807185c4c97d01af6c05728264dacb6a3c8b1ef927341a88c09
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: toolkit-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://af08b676bd435807185c4c97d01af6c05728264dacb6a3c8b1ef927341a88c09
          exitCode: 0
          finishedAt: "2023-03-02T16:15:40Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:25Z"
    phase: Running
    podIP: 10.131.2.15
    podIPs:
    - ip: 10.131.2.15
    qosClass: BestEffort
    startTime: "2023-03-02T16:14:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.20"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.20"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-02T16:15:48Z"
    generateName: nvidia-device-plugin-validator-
    labels:
      app: nvidia-device-plugin-validator
    name: nvidia-device-plugin-validator-tjd77
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: nvidia.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ClusterPolicy
      name: ocp-gpu-addon
      uid: eaa7f72e-7226-4afc-8d7e-ca5787b7a154
    resourceVersion: "973016"
    uid: 6a22172d-2a16-4ef6-86f0-5d2ebbff2a75
  spec:
    containers:
    - args:
      - echo device-plugin workload validation is successful
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: nvidia-device-plugin-validator
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lzsm2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nvidia-operator-validator-dockercfg-fd76m
    initContainers:
    - args:
      - vectorAdd
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: plugin-validation
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lzsm2
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: nvidia-operator-validator
    serviceAccountName: nvidia-operator-validator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-lzsm2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:51Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:48Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:48Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://cfa6a0a51084a5955c17308e5a79952d80766dca2cd96b82d9aa60c8e96039f1
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: nvidia-device-plugin-validator
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://cfa6a0a51084a5955c17308e5a79952d80766dca2cd96b82d9aa60c8e96039f1
          exitCode: 0
          finishedAt: "2023-03-02T16:15:51Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:51Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://4e55b002c7de17a1fc824b727637082c6c2753ba445cd3e4ee32dd1af66ddb47
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: plugin-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://4e55b002c7de17a1fc824b727637082c6c2753ba445cd3e4ee32dd1af66ddb47
          exitCode: 0
          finishedAt: "2023-03-02T16:15:51Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:50Z"
    phase: Succeeded
    podIP: 10.131.2.20
    podIPs:
    - ip: 10.131.2.20
    qosClass: BestEffort
    startTime: "2023-03-02T16:15:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.13"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.13"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-driver
    creationTimestamp: "2023-03-01T12:39:18Z"
    generateName: nvidia-driver-daemonset-410.84.202302080019-0-
    labels:
      app: nvidia-driver-daemonset-410.84.202302080019-0
      controller-revision-hash: 55dd87b668
      openshift.driver-toolkit: "true"
      pod-template-generation: "1"
    name: nvidia-driver-daemonset-410.84.202302080019-0-4h2pq
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-driver-daemonset-410.84.202302080019-0
      uid: a6e58221-556f-4dbd-8124-0abb15af58df
    resourceVersion: "971538"
    uid: cedd95d1-5686-47ca-8ae2-6876b3f14ec1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - args:
      - nv-ctr-run-with-dtk
      command:
      - ocp_dtk_entrypoint
      env:
      - name: RHEL_VERSION
        value: "8.4"
      - name: OPENSHIFT_VERSION
        value: "4.10"
      image: nvcr.io/nvidia/driver@sha256:268c26781c46b36fe691638e807eb5c3a2cd077029ddb095ac75264adac04925
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /run/nvidia/validations/.driver-ctr-ready
      name: nvidia-driver-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      startupProbe:
        exec:
          command:
          - sh
          - -c
          - nvidia-smi && touch /run/nvidia/validations/.driver-ctr-ready
        failureThreshold: 120
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: run-nvidia
      - mountPath: /run/nvidia-topologyd
        name: run-nvidia-topologyd
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /run/mellanox/drivers/usr/src
        mountPropagation: HostToContainer
        name: mlnx-ofed-usr-src
      - mountPath: /run/mellanox/drivers
        mountPropagation: HostToContainer
        name: run-mellanox-drivers
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gmd2k
        readOnly: true
    - args:
      - until [ -f /mnt/shared-nvidia-driver-toolkit/dir_prepared ]; do echo  Waiting
        for nvidia-driver-ctr container to prepare the shared directory ...; sleep
        10; done; exec /mnt/shared-nvidia-driver-toolkit/ocp_dtk_entrypoint dtk-build-driver
      command:
      - bash
      - -xc
      env:
      - name: RHCOS_VERSION
        value: 410.84.202302080019-0
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f9c224853c0b4ae241b4d9bebd84ed2dff9b92fdbd17b79bb750b5a13ede734
      imagePullPolicy: IfNotPresent
      name: openshift-driver-toolkit-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/log
        name: var-log
      - mountPath: /run/mellanox/drivers/usr/src
        mountPropagation: HostToContainer
        name: mlnx-ofed-usr-src
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gmd2k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostPID: true
    imagePullSecrets:
    - name: nvidia-driver-dockercfg-zm9wc
    initContainers:
    - args:
      - uninstall_driver
      command:
      - driver-manager
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      - name: ENABLE_GPU_POD_EVICTION
        value: "true"
      - name: ENABLE_AUTO_DRAIN
        value: "true"
      - name: DRAIN_USE_FORCE
        value: "false"
      - name: DRAIN_POD_SELECTOR_LABEL
      - name: DRAIN_TIMEOUT_SECONDS
        value: 0s
      - name: DRAIN_DELETE_EMPTYDIR_DATA
        value: "false"
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:9177a0ae30798b42d0387f6a20cd3ce3cd1799a91b7866bf812368764b05b1af
      imagePullPolicy: IfNotPresent
      name: k8s-driver-manager
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: run-nvidia
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /sys
        name: host-sys
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gmd2k
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      feature.node.kubernetes.io/system-os_release.OSTREE_VERSION: 410.84.202302080019-0
      nvidia.com/gpu.deploy.driver: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-driver
    serviceAccountName: nvidia-driver
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: DirectoryOrCreate
      name: run-nvidia
    - hostPath:
        path: /var/log
        type: ""
      name: var-log
    - hostPath:
        path: /dev/log
        type: ""
      name: dev-log
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /run/nvidia-topologyd
        type: DirectoryOrCreate
      name: run-nvidia-topologyd
    - hostPath:
        path: /run/mellanox/drivers/usr/src
        type: DirectoryOrCreate
      name: mlnx-ofed-usr-src
    - hostPath:
        path: /run/mellanox/drivers
        type: DirectoryOrCreate
      name: run-mellanox-drivers
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /sys
        type: Directory
      name: host-sys
    - emptyDir: {}
      name: shared-nvidia-driver-toolkit
    - name: kube-api-access-gmd2k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:39:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://39409bd84fc10f2dc61175acd57f1f05127b47a9351e580c5db6a521845424fd
      image: nvcr.io/nvidia/driver@sha256:268c26781c46b36fe691638e807eb5c3a2cd077029ddb095ac75264adac04925
      imageID: nvcr.io/nvidia/driver@sha256:268c26781c46b36fe691638e807eb5c3a2cd077029ddb095ac75264adac04925
      lastState: {}
      name: nvidia-driver-ctr
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:22Z"
    - containerID: cri-o://bf98a0d4e27e35f072c76b0f1eb7a637a458a112c60c41dc4861fb9a0f5d5fc1
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f9c224853c0b4ae241b4d9bebd84ed2dff9b92fdbd17b79bb750b5a13ede734
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6f9c224853c0b4ae241b4d9bebd84ed2dff9b92fdbd17b79bb750b5a13ede734
      lastState: {}
      name: openshift-driver-toolkit-ctr
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:22Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://19abe180e9d81ee273a9479fedcffe8aa86ed4321f7a1a49f78f181965e3245c
      image: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:9177a0ae30798b42d0387f6a20cd3ce3cd1799a91b7866bf812368764b05b1af
      imageID: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:06482db4739b5292fdfda2b6efd4e758c1f0dc9bf2f4e569a2ed971cd71fffd4
      lastState: {}
      name: k8s-driver-manager
      ready: true
      restartCount: 2
      state:
        terminated:
          containerID: cri-o://19abe180e9d81ee273a9479fedcffe8aa86ed4321f7a1a49f78f181965e3245c
          exitCode: 0
          finishedAt: "2023-03-02T16:14:21Z"
          reason: Completed
          startedAt: "2023-03-02T16:13:42Z"
    phase: Running
    podIP: 10.131.2.13
    podIPs:
    - ip: 10.131.2.13
    qosClass: BestEffort
    startTime: "2023-03-01T12:39:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.4"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.4"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-driver
    creationTimestamp: "2023-03-01T12:39:19Z"
    generateName: nvidia-node-status-exporter-
    labels:
      app: nvidia-node-status-exporter
      controller-revision-hash: 7799d44fcf
      pod-template-generation: "1"
    name: nvidia-node-status-exporter-qxfw2
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-node-status-exporter
      uid: d0dfb89f-a77d-4e71-a9a5-aaf02e02d67a
    resourceVersion: "964038"
    uid: 620ff653-7288-4e95-a52a-2b846195f4dc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - command:
      - nvidia-validator
      env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      - name: COMPONENT
        value: metrics
      - name: METRICS_PORT
        value: "8000"
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: nvidia-node-status-exporter
      ports:
      - containerPort: 8000
        name: node-status
        protocol: TCP
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2259l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nvidia-node-status-exporter-dockercfg-lch98
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.node-status-exporter: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-node-status-exporter
    serviceAccountName: nvidia-node-status-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - hostPath:
        path: /
        type: ""
      name: host-root
    - name: kube-api-access-2259l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:39:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:13:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:39:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://d3965f4741eabef1c6ad5f98d0137b83edf9add8dcfb748701ec65989e73c949
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: nvidia-node-status-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:13:39Z"
    hostIP: 10.0.193.255
    phase: Running
    podIP: 10.131.2.4
    podIPs:
    - ip: 10.131.2.4
    qosClass: BestEffort
    startTime: "2023-03-01T12:39:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.17"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.131.2.17"
            ],
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-operator-validator
    creationTimestamp: "2023-03-02T16:14:23Z"
    generateName: nvidia-operator-validator-
    labels:
      app: nvidia-operator-validator
      app.kubernetes.io/part-of: gpu-operator
      controller-revision-hash: 6c5f84d4d4
      pod-template-generation: "1"
    name: nvidia-operator-validator-knv7n
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-operator-validator
      uid: 682a9f97-7f3b-4f69-94fc-80aa90d5507a
    resourceVersion: "973165"
    uid: 7c145db9-9027-48d2-8a88-417c8d42506f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-10-0-193-255.us-east-2.compute.internal
    containers:
    - args:
      - echo all validations are successful; sleep infinity
      command:
      - sh
      - -c
      env:
      - name: WITH_WORKLOAD
        value: "true"
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /run/nvidia/validations/*-ready
      name: nvidia-operator-validator
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m9lxr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: nvidia-operator-validator-dockercfg-fd76m
    initContainers:
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "true"
      - name: COMPONENT
        value: driver
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: driver-validation
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /run/nvidia/driver
        mountPropagation: HostToContainer
        name: driver-install-path
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /host-dev-char
        name: host-dev-char
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m9lxr
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "false"
      - name: COMPONENT
        value: toolkit
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m9lxr
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "false"
      - name: COMPONENT
        value: cuda
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VALIDATOR_IMAGE
        value: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      - name: VALIDATOR_IMAGE_PULL_POLICY
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: cuda-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m9lxr
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: COMPONENT
        value: plugin
      - name: WITH_WAIT
        value: "false"
      - name: WITH_WORKLOAD
        value: "true"
      - name: MIG_STRATEGY
        value: single
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VALIDATOR_IMAGE
        value: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      - name: VALIDATOR_IMAGE_PULL_POLICY
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imagePullPolicy: IfNotPresent
      name: plugin-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m9lxr
        readOnly: true
    nodeName: ip-10-0-193-255.us-east-2.compute.internal
    nodeSelector:
      nvidia.com/gpu.deploy.operator-validator: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-operator-validator
    serviceAccountName: nvidia-operator-validator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /run/nvidia/driver
        type: ""
      name: driver-install-path
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /dev/char
        type: ""
      name: host-dev-char
    - name: kube-api-access-m9lxr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:15:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:16:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:16:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://ac429ddcfa9867c4c7b336aa20bd49e3039c824f8fbb9c093bdd00608af435a1
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: nvidia-operator-validator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:15:59Z"
    hostIP: 10.0.193.255
    initContainerStatuses:
    - containerID: cri-o://9e4ddf024ac78babd14bdfddafc6a75df3bf3de3b4315976359dabfdb63f4005
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: driver-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://9e4ddf024ac78babd14bdfddafc6a75df3bf3de3b4315976359dabfdb63f4005
          exitCode: 0
          finishedAt: "2023-03-02T16:15:13Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:27Z"
    - containerID: cri-o://7f366ee25ed690ad59c46e61e3ec1d30a497a4bff900fbb5c6c88f14e7ec6716
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: toolkit-validation
      ready: true
      restartCount: 2
      state:
        terminated:
          containerID: cri-o://7f366ee25ed690ad59c46e61e3ec1d30a497a4bff900fbb5c6c88f14e7ec6716
          exitCode: 0
          finishedAt: "2023-03-02T16:15:37Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:36Z"
    - containerID: cri-o://79da170240581ac434f3c92d8606d5998bc90d53cd216c2c67f7f7af5752d7bb
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: cuda-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://79da170240581ac434f3c92d8606d5998bc90d53cd216c2c67f7f7af5752d7bb
          exitCode: 0
          finishedAt: "2023-03-02T16:15:47Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:37Z"
    - containerID: cri-o://e08f057442311e097e0d96c016aebdb401b8640fbd10488a1fefe920266fd555
      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      imageID: nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:00f1476548fbed9ee01961443a73bf65396c2e8bb2b84426f949dd56cb4d14cd
      lastState: {}
      name: plugin-validation
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: cri-o://e08f057442311e097e0d96c016aebdb401b8640fbd10488a1fefe920266fd555
          exitCode: 0
          finishedAt: "2023-03-02T16:15:58Z"
          reason: Completed
          startedAt: "2023-03-02T16:15:48Z"
    phase: Running
    podIP: 10.131.2.17
    podIPs:
    - ip: 10.131.2.17
    qosClass: BestEffort
    startTime: "2023-03-02T16:14:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.2"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.2"
            ],
            "default": true,
            "dns": {}
        }]
      kubectl.kubernetes.io/default-container: prometheus
      openshift.io/scc: restricted
    creationTimestamp: "2023-03-01T12:37:59Z"
    generateName: prometheus-gpuaddon-prometheus-
    labels:
      app.kubernetes.io/instance: gpuaddon-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.36.0
      controller-revision-hash: prometheus-gpuaddon-prometheus-7ccbb99795
      operator.prometheus.io/name: gpuaddon-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: gpuaddon-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-gpuaddon-prometheus-0
    name: prometheus-gpuaddon-prometheus-0
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-gpuaddon-prometheus
      uid: 5c8e7054-3ed0-458f-8e2c-11cdec442bad
    resourceVersion: "967630"
    uid: f0466897-3887-422d-8b93-f3318f30b67d
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=24h
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --web.listen-address=127.0.0.1:9090
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: registry.redhat.io/openshift4/ose-prometheus@sha256:965be94ef56057f39551a4925e92b406deabcc8acad3d9ba3f44a55e77fd153a
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - if [ -x "$(command -v curl)" ]; then exec curl --fail http://localhost:9090/-/healthy;
            elif [ -x "$(command -v wget)" ]; then exec wget -q -O /dev/null http://localhost:9090/-/healthy;
            else exit 1; fi
        failureThreshold: 6
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - if [ -x "$(command -v curl)" ]; then exec curl --fail http://localhost:9090/-/ready;
            elif [ -x "$(command -v wget)" ]; then exec wget -q -O /dev/null http://localhost:9090/-/ready;
            else exit 1; fi
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "1"
          memory: 250Mi
        requests:
          cpu: "1"
          memory: 250Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      startupProbe:
        exec:
          command:
          - sh
          - -c
          - if [ -x "$(command -v curl)" ]; then exec curl --fail http://localhost:9090/-/ready;
            elif [ -x "$(command -v wget)" ]; then exec wget -q -O /dev/null http://localhost:9090/-/ready;
            else exit 1; fi
        failureThreshold: 60
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-gpuaddon-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-gpuaddon-prometheus-rulefiles-0
        name: prometheus-gpuaddon-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m8zcz
        readOnly: true
    - args:
      - --listen-address=localhost:8080
      - --reload-url=http://localhost:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-gpuaddon-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imagePullPolicy: IfNotPresent
      name: config-reloader
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-gpuaddon-prometheus-rulefiles-0
        name: prometheus-gpuaddon-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m8zcz
        readOnly: true
    - args:
      - --secure-listen-address=0.0.0.0:9339
      - --upstream=http://127.0.0.1:9090/
      - --logtostderr=true
      - --v=10
      - --tls-cert-file=/etc/tls-secret/tls.crt
      - --tls-private-key-file=/etc/tls-secret/tls.key
      - --client-ca-file=/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
      - --config-file=/etc/kube-rbac-config/config-file.json
      image: quay.io/openshift/origin-kube-rbac-proxy:4.10.0
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9339
        name: https
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls-secret
        name: serving-cert
      - mountPath: /etc/kube-rbac-config
        name: kube-rbac-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m8zcz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-gpuaddon-prometheus-0
    imagePullSecrets:
    - name: prometheus-k8s-dockercfg-bvdx7
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-gpuaddon-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        readOnlyRootFilesystem: true
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-gpuaddon-prometheus-rulefiles-0
        name: prometheus-gpuaddon-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m8zcz
        readOnly: true
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: prometheus-k8s
    serviceAccountName: prometheus-k8s
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-gpuaddon-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-gpuaddon-prometheus-tls-assets-0
    - emptyDir: {}
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-gpuaddon-prometheus-rulefiles-0
      name: prometheus-gpuaddon-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-gpuaddon-prometheus-web-config
    - emptyDir: {}
      name: prometheus-gpuaddon-prometheus-db
    - name: serving-cert
      secret:
        defaultMode: 420
        secretName: prometheus-serving-cert-secret
    - configMap:
        defaultMode: 420
        name: prometheus-kube-rbac-proxy-config
      name: kube-rbac-config
    - name: kube-api-access-m8zcz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:38:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://02aa8480daa21c8aafdd9078b70e17a61152543e8b862cdf23f212ec083838dc
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imageID: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:06Z"
    - containerID: cri-o://c3a9e2c4ffe76a99863e437c9da5d29399e7e2b5368f05f874279d948fa2d732
      image: quay.io/openshift/origin-kube-rbac-proxy:4.10.0
      imageID: quay.io/openshift/origin-kube-rbac-proxy@sha256:baedb268ac66456018fb30af395bb3d69af5fff3252ff5d549f0231b1ebb6901
      lastState: {}
      name: kube-rbac-proxy
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:08Z"
    - containerID: cri-o://f7d547b885ff5662f1335f71e3fa1604a6f9c113993a657e9487479ad79b7a78
      image: registry.redhat.io/openshift4/ose-prometheus@sha256:965be94ef56057f39551a4925e92b406deabcc8acad3d9ba3f44a55e77fd153a
      imageID: registry.redhat.io/openshift4/ose-prometheus@sha256:1776971d340a724503881408e2cb795c84adfe718a9ed03028179a0e5fbc2870
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:06Z"
    hostIP: 10.0.197.168
    initContainerStatuses:
    - containerID: cri-o://f5c3aab0c26045eb092211794866b58f94f9c9aa00aa0d18e48c8ce28032ac1b
      image: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      imageID: registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: cri-o://f5c3aab0c26045eb092211794866b58f94f9c9aa00aa0d18e48c8ce28032ac1b
          exitCode: 0
          finishedAt: "2023-03-02T16:14:04Z"
          reason: Completed
          startedAt: "2023-03-02T16:14:04Z"
    phase: Running
    podIP: 10.128.2.2
    podIPs:
    - ip: 10.128.2.2
    qosClass: Burstable
    startTime: "2023-03-01T12:37:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      capabilities: Deep Insights
      categories: Monitoring
      certified: "false"
      containerImage: registry.redhat.io/openshift4/ose-prometheus-operator:v4.10.0-202204090935.p0.g73ddd44.assembly.stream
      createdAt: "2021-04-15T23:43:00Z"
      description: Manage the full lifecycle of configuring and managing Prometheus
        and Alertmanager servers.
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.3"
            ],
            "default": true,
            "dns": {}
        }]
      k8s.v1.cni.cncf.io/networks-status: |-
        [{
            "name": "openshift-sdn",
            "interface": "eth0",
            "ips": [
                "10.128.2.3"
            ],
            "default": true,
            "dns": {}
        }]
      olm.operatorGroup: redhat-layered-product-og
      olm.operatorNamespace: redhat-nvidia-gpu-addon
      olm.targetNamespaces: redhat-nvidia-gpu-addon
      openshift.io/scc: restricted
      operatorframework.io/properties: '{"properties":[{"type":"olm.package","value":{"packageName":"ose-prometheus-operator","version":"4.10.0"}}]}'
      repository: https://github.com/prometheus-operator/prometheus-operator
      support: Red Hat, Inc.
    creationTimestamp: "2023-03-01T12:37:46Z"
    generateName: prometheus-operator-7d5889b6-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/name: prometheus-operator
      app.kubernetes.io/version: 4.10.0
      k8s-app: prometheus-operator
      pod-template-hash: 7d5889b6
    name: prometheus-operator-7d5889b6-82hxl
    namespace: redhat-nvidia-gpu-addon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-operator-7d5889b6
      uid: c320afb8-da93-4fd3-841c-f4547d17f796
    resourceVersion: "966687"
    uid: ef16f8f2-e78e-438f-9748-c40446998720
  spec:
    containers:
    - args:
      - -namespaces=$(NAMESPACES)
      - --prometheus-config-reloader=registry.redhat.io/openshift4/ose-prometheus-config-reloader@sha256:2a953b6a38c5c59376165afa8d7c46bdf586ce2d3bf59f745130e4d09a0813a6
      - --prometheus-default-base-image=registry.redhat.io/openshift4/ose-prometheus@sha256:965be94ef56057f39551a4925e92b406deabcc8acad3d9ba3f44a55e77fd153a
      - --alertmanager-default-base-image=registry.redhat.io/openshift4/ose-prometheus-alertmanager@sha256:c19d7f46cdbe73cdec06fde1bb621180e512400145d8aff94476a464004b6dd8
      env:
      - name: NAMESPACES
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.annotations['olm.targetNamespaces']
      - name: OPERATOR_CONDITION_NAME
        value: ose-prometheus-operator.4.10.0
      image: registry.redhat.io/openshift4/ose-prometheus-operator@sha256:ae1771e6a79e669b4b9a852e3ce75ff9230a57479c8508201de5d923a27d2cca
      imagePullPolicy: IfNotPresent
      name: prometheus-operator
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - KILL
          - MKNOD
          - SETGID
          - SETUID
        runAsUser: 1001040000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-64q2q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: prometheus-operator-dockercfg-jml8v
    nodeName: ip-10-0-197-168.us-east-2.compute.internal
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001040000
      seLinuxOptions:
        level: s0:c32,c24
    serviceAccount: prometheus-operator
    serviceAccountName: prometheus-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: kube-api-access-64q2q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-02T16:14:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T12:37:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://3092bc803b0b4c19355c2326256b4a69de3c5216bc3f3bd358287b027674ab8a
      image: registry.redhat.io/openshift4/ose-prometheus-operator@sha256:ae1771e6a79e669b4b9a852e3ce75ff9230a57479c8508201de5d923a27d2cca
      imageID: registry.redhat.io/openshift4/ose-prometheus-operator@sha256:9bd7afee5a03056dbcde59077180187793cb0d47ca250a2206c9806464d662ab
      lastState: {}
      name: prometheus-operator
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-02T16:14:04Z"
    hostIP: 10.0.197.168
    phase: Running
    podIP: 10.128.2.3
    podIPs:
    - ip: 10.128.2.3
    qosClass: Burstable
    startTime: "2023-03-01T12:37:46Z"
kind: List
metadata:
  resourceVersion: ""
